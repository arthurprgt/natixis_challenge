{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "In this notebook, we review the dataset to understand what the data represent and the relationship between data elements. This also includes dealing with the null values and defining the preprocessing function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1° Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../RFQ_Data_Challenge_HEC.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2° Defining the preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at null values : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values analysis for columns with null values below 15000\n",
    "below_threshold = df.isnull().sum().sort_values(ascending=False) < 15000\n",
    "print(\"Columns with null values below 15000:\")\n",
    "print(below_threshold[below_threshold].index)\n",
    "\n",
    "# Null values analysis for columns with null values above 15000\n",
    "above_threshold = df.isnull().sum().sort_values(ascending=False) >= 15000\n",
    "print(\"\\nColumns with null values above or equal to 15000:\")\n",
    "print(above_threshold[above_threshold].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And column's types :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First insights on features :\n",
    "- **Deal_Date** : The date on which a financial deal is executed. Needs to be converteted in datetime.\n",
    "- **ISIN** : International Securities Identification Number, a unique identifier for Financial instruments.\n",
    "- **company_short_name** : the name of the issuer of the financial instrument. It corresponds to the client name.\n",
    "- **B_price** : The bid price of the financial instrument. To be converted to int (object for the moment)\n",
    "- **B_side** : Natixis's position as a buyer or seller of the financial instrument (for the moment 'NATIXIS SELL' or 'NATIXIS BUY'). Contient 8 valeurs nulles.\n",
    "- **Total_Requested_Volume** : The requested volume for buying or selling the financial instrument. It needs to be converted into a numerical column (object for the moment). Contient 2 valeurs nulles.\n",
    "- **Total_Traded_Volume_Natixis** : The volume of the financial instrument traded by Natixis. Already good data type.\n",
    "- **Total_Traded_Volume_Away** : The volume of the financial instrument traded by other banks. Already good data type.\n",
    "- **Total_Traded_Volume** : The total volume of the financial instrument traded. Already good data type.\n",
    "- **BloomIndustrySector**, **BloomIndustryGroup**,**BloomIndustrySubGroup**\n",
    "- **maturity** : The length of time during which interest is paid. Some null values are marked as NaT. We convert this column into Datetime type. Some maturities go back to 1900, it is not possible. We delete those.\n",
    "- **Rating_Fitch** : The credit rating of the financial instrument from Fitch Ratings.\n",
    "- **Rating_Moodys** : The credit rating of the financial instrument from Moody's.\n",
    "- **Rating_SP** : The credit rating of the financial instrument from S&P Global Ratings.\n",
    "- **Ccy** : The currency in which the financial instrument is denominated.\n",
    "- **Classification** : The activity sector of the company.\n",
    "- **Tier** : The seniority level of the financial instrument. Lots of null value, we replace them by UNKOWN (627100 values)\n",
    "- **AssumedMaturity** : The assumed maturity date of the financial instrument. Also contains a lot of null values, we replace them by maturity values (the null values only).\n",
    "- **Coupon** : The interest rate of the financial instrument. Already a float.\n",
    "- **Frequency** : The frequency of interest payments on the financial instrument. Takes values 1M, 3M, 6M, 12M. We delete the 'M' and convert the value into int.\n",
    "- **Type** : The type of interest rate on the financial instrument (fixed or variable).\n",
    "- **MidYTM** : The yield to maturity on the prime bid. Already a float.\n",
    "- **MidYTM** : The yield to maturity on the prime bid. Already a float.\n",
    "- **YTWDate** : Yield to Worst - The yield on the first possible redemption date. \n",
    "- **SpreadvsBenchmarkMid** : The spread of the financial instrument versus the interpolated government bond curve.\n",
    "- **MidASWSpread** : The spread of the financial instrument versus the swap curve.\n",
    "- **MidZSpread** : The spread of the financial instrument versus the zero- coupon curve.\n",
    "- **GSpreadMid** : The spread of the financial instrument versus the interpolated government bond curve.\n",
    "- **MidModifiedDuration** : The modified duration of the financial instrument. \n",
    "- **MidConvexity** : The convexity of the financial instrument.\n",
    "- **MidEffectiveDuration** : The effective duration of the financial instrument.\n",
    "- **MidEffectiveConvexity** : The effective convexity of the financial instrument.\n",
    "\n",
    "Features that can be deleted at first : \n",
    "- **Cusip**, same as **cusip** but with more null values \n",
    "- **Maturity**, same as **maturity**\n",
    "- **YTWDate**, too many missing values \n",
    "\n",
    "Added columns :\n",
    "- Year, month, day of deal_date\n",
    "- Year, month, day of maturity\n",
    "- days to maturity\n",
    "- have_YTWDate : if YTWDate is not null, have_YTW takes the value 1, otherwise 0\n",
    "\n",
    "Remark : Of the zero values in the YTWDate column, almost 100% correspond to government bonds. One possible reason is that government bonds can be issued without an early redemption clause, meaning that there is no date on which the issuer can redeem the bond before maturity. In this case, the YTWDate column would have no significant value and could be left blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame with the following steps:\n",
    "    1. Converts 'Deal_Date', 'maturity', 'AssumedMaturity', 'YTWDate' columns to datetime.\n",
    "    2. Converts 'B_Side' column to boolean (1 for 'NATIXIS BUY', 0 for 'NATIXIS SELL').\n",
    "    3. Converts 'B_Price' and 'Total_Requested_Volume' columns to integers.\n",
    "    4. Fills null values in 'Tier', 'AssumedMaturity', and 'YTWDate' columns with 'UNKNOWN'.\n",
    "    5. Converts 'Frequency' feature values into integers (removing 'M' from the end).\n",
    "    6. Drops the unsused 'Cusip' column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop null values only for columns below the threshold\n",
    "    columns_to_delete_null_vales = ['MidYTM', 'Coupon', 'Ccy', 'cusip',\n",
    "       'maturity', 'cdcissuerShortName', 'Frequency', 'MidPrice', 'cdcissuer',\n",
    "       'company_short_name', 'BloomIndustrySubGroup', 'B_Price',\n",
    "       'Total_Traded_Volume_Natixis', 'B_Side',\n",
    "       'Total_Traded_Volume_Away', 'Total_Requested_Volume',\n",
    "       'Total_Traded_Volume', 'Type', 'Maturity', 'ISIN', 'Deal_Date']\n",
    "    df = df.dropna(subset=columns_to_delete_null_vales)\n",
    "\n",
    "    # Convert 'B_Price', 'Total_Requested_Volume', 'Frequency' to integers\n",
    "    df['Frequency'] = df['Frequency'].str.replace('M', '')\n",
    "    numerical_columns = ['B_Price', 'Total_Requested_Volume', 'Frequency']\n",
    "    df.dropna(subset=numerical_columns, inplace=True)\n",
    "    for column in numerical_columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce').astype(int)\n",
    "\n",
    "    # Fix the error in the B_Price column\n",
    "    df = df[df['B_Price'] >= 20]\n",
    "\n",
    "    # Replace NaT with null values in the 'Maturity' column\n",
    "    df['maturity'].replace({pd.NaT: np.nan}, inplace=True)\n",
    "\n",
    "    # Convert 'Deal_Date', 'maturity', 'AssumedMaturity', 'YTWDate' to datetime\n",
    "    df['Deal_Date'] = pd.to_datetime(df['Deal_Date'])\n",
    "    df['maturity'] = pd.to_datetime(df['maturity'], errors='coerce',  format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df['AssumedMaturity'] = pd.to_datetime(df['AssumedMaturity'], errors='coerce')\n",
    "    df['YTWDate'] = pd.to_datetime(df['YTWDate'], errors='coerce')\n",
    "\n",
    "    # Add year, month, day for clustering \n",
    "    df['Year_dealdate'] = df['Deal_Date'].dt.year\n",
    "    df['Month_dealdate'] = df['Deal_Date'].dt.month\n",
    "    df['Day_dealdate'] = df['Deal_Date'].dt.day\n",
    "    df['Year_maturity'] = df['maturity'].dt.year\n",
    "    df['Month_maturity'] = df['maturity'].dt.month\n",
    "    df['Day_maturity'] = df['maturity'].dt.day\n",
    "\n",
    "    # Delete maturities smaller than 2021 (as deal dates starts in 2021)\n",
    "    df = df[df['maturity'].dt.year >= 2021]\n",
    "\n",
    "    # Compute number of days between maturity and deal date\n",
    "    df['Days_to_Maturity'] = (df['maturity'] - df['Deal_Date']).dt.days\n",
    "\n",
    "    # Replace null values in 'AssumedMaturity' with values from 'Maturity'\n",
    "    df['AssumedMaturity'] = df['AssumedMaturity'].fillna(df['Maturity'])\n",
    "\n",
    "    # Convert 'B_Side' column to boolean (1 for 'NATIXIS BUY', 0 for 'NATIXIS SELL')\n",
    "    df = df[df['B_Side'].isin(['NATIXIS SELL', 'NATIXIS BUY'])]\n",
    "    df['B_Side'] = df['B_Side'].replace({'NATIXIS BUY': 1, 'NATIXIS SELL': 0})\n",
    "\n",
    "    # Lower string names \n",
    "    df['Sales_Name'] = df['Sales_Name'].str.lower()\n",
    "    df['company_short_name'] = df['company_short_name'].str.lower()\n",
    "\n",
    "    # Clean Ratings \n",
    "    df['Rating_Moodys'] = df['Rating_Moodys'].str.replace('(P)', '')\n",
    "\n",
    "    # Replace some nan values by 'UNKNOWN'\n",
    "    columns_to_fill_unknown = ['Tier', 'lb_Platform_2', 'Country', 'Rating_SP', 'Rating_Fitch', 'Rating_Moodys', 'Sales_Name', 'Sales_Initial', 'Instrument']\n",
    "    df[columns_to_fill_unknown] = df[columns_to_fill_unknown].fillna('UNKNOWN')\n",
    "\n",
    "    # Drop unused columns\n",
    "    df['have_YTW'] = df['YTWDate'].notnull().astype(int) # If YTWDate is not null, have_YTW takes the value 1, otherwise 0\n",
    "    columns_to_drop = ['Cusip', 'Maturity', 'YTWDate']\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = preprocess_dataframe(df)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 314718 transactions left, once we've corrected the B_price error, which sometimes corresponds to yield values (by setting a minimum value of 20) and deleted some null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we look at how to complete the null values of the numerical columns : 'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', 'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity'... As there are a lot. We would like to replace the nan values in those rows by the mean either by 'BloomIndustrySector' or 'Classification', at yearly time granularity. Let's see which replacement should be the more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select = ['ISIN', 'Deal_Date', 'Classification',  'BloomIndustrySector', 'BloomIndustryGroup',\n",
    "                     'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                     'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity',\n",
    "                     'Year_dealdate', 'Month_dealdate']\n",
    "df_nan_completition = df_preprocessed[columns_to_select]\n",
    "df_nan_completition = df_nan_completition.groupby('ISIN').first()\n",
    "df_nan_completition.sort_values(by='Deal_Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's study the 'Classification' groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe, Group by Classification\n",
    "df_by_classification = df_nan_completition[['Classification', \n",
    "                     'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                     'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity', 'Year_dealdate', 'Month_dealdate']]\n",
    "df_by_classification = df_by_classification.groupby(['Classification', 'Year_dealdate', 'Month_dealdate']).mean().reset_index()\n",
    "df_by_classification['Deal_Date'] = df_by_classification['Year_dealdate'].astype(str) + '-' + df_by_classification['Month_dealdate'].astype(str) + '-01'\n",
    "df_by_classification['Deal_Date'] = pd.to_datetime(df_by_classification['Deal_Date'])\n",
    "df_by_classification = df_by_classification.drop(['Year_dealdate', 'Month_dealdate'], axis=1)\n",
    "\n",
    "# Grouping by classification and by year, then describing\n",
    "numeric_columns = ['SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid',\n",
    "                   'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity']\n",
    "summary_by_classification_year = df_by_classification.groupby(['Classification', df_by_classification['Deal_Date'].dt.year])[numeric_columns].describe()\n",
    "summary_by_classification_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphic analysis\n",
    "\n",
    "def plot_numerical_columns_by_classification(df_nan_completition):\n",
    "    # Process the dataframe\n",
    "    df_by_classification = df_nan_completition[['Classification', \n",
    "                        'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                        'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity', 'Year_dealdate', 'Month_dealdate']]\n",
    "    df_by_classification = df_by_classification.groupby(['Classification', 'Year_dealdate', 'Month_dealdate']).mean().reset_index()\n",
    "\n",
    "    df_by_classification['Deal_Date'] = df_by_classification['Year_dealdate'].astype(str) + '-' + df_by_classification['Month_dealdate'].astype(str) + '-01'\n",
    "    df_by_classification['Deal_Date'] = pd.to_datetime(df_by_classification['Deal_Date'])\n",
    "    df_by_classification = df_by_classification.drop(['Year_dealdate', 'Month_dealdate'], axis=1)\n",
    "\n",
    "    # List of numerical columns\n",
    "    numeric_columns = ['SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                    'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity']\n",
    "\n",
    "    classifications = df_by_classification.Classification.unique()\n",
    "\n",
    "    # Generating the plots\n",
    "    for column in numeric_columns:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        for classification in classifications:\n",
    "            df_filtered = df_by_classification[df_by_classification['Classification'] == classification]\n",
    "            plt.plot(df_filtered['Deal_Date'], df_filtered[column], label=classification)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.legend(title='Classification', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.title(f'{column} Over Time for Different Classifications')\n",
    "        plt.xlabel('Deal Date')\n",
    "        plt.ylabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plot_numerical_columns_by_classification(df_nan_completition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's study the results obtained with using a 'BloomIndustrySector' groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe, group by Industry sector\n",
    "df_by_industrySector = df_nan_completition[['BloomIndustrySector', \n",
    "                     'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                     'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity', 'Year_dealdate', 'Month_dealdate']]\n",
    "df_by_industrySector = df_by_industrySector.groupby(['BloomIndustrySector', 'Year_dealdate', 'Month_dealdate']).mean().reset_index()\n",
    "df_by_industrySector['Deal_Date'] = df_by_industrySector['Year_dealdate'].astype(str) + '-' + df_by_industrySector['Month_dealdate'].astype(str) + '-01'\n",
    "df_by_industrySector['Deal_Date'] = pd.to_datetime(df_by_classification['Deal_Date'])\n",
    "df_by_industrySector = df_by_industrySector.drop(['Year_dealdate', 'Month_dealdate'], axis=1)\n",
    "\n",
    "# Grouping by industry sector and by year, then showing statistical summary\n",
    "summary_by_industry_year = df_by_industrySector.groupby(['BloomIndustrySector', df_by_classification['Deal_Date'].dt.year])[numeric_columns].describe()\n",
    "summary_by_industry_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphic analysis\n",
    "\n",
    "def plot_numerical_columns_by_industry(df_nan_completition):\n",
    "\n",
    "    # Process the dataframe\n",
    "    df_by_industrySector = df_nan_completition[['BloomIndustrySector', \n",
    "                        'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                        'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity', 'Year_dealdate', 'Month_dealdate']]\n",
    "    df_by_industrySector = df_by_industrySector.groupby(['BloomIndustrySector', 'Year_dealdate', 'Month_dealdate']).mean().reset_index()\n",
    "\n",
    "    df_by_industrySector['Deal_Date'] = df_by_industrySector['Year_dealdate'].astype(str) + '-' + df_by_industrySector['Month_dealdate'].astype(str) + '-01'\n",
    "    df_by_industrySector['Deal_Date'] = pd.to_datetime(df_by_industrySector['Deal_Date'])\n",
    "    df_by_industrySector = df_by_industrySector.drop(['Year_dealdate', 'Month_dealdate'], axis=1)\n",
    "\n",
    "    # Liste des colonnes numériques\n",
    "    numeric_columns = ['SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                    'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity']\n",
    "\n",
    "    industries = df_by_industrySector.BloomIndustrySector.unique()\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        for industry in industries:\n",
    "            df_filtered = df_by_industrySector[df_by_industrySector['BloomIndustrySector'] == industry]\n",
    "            plt.plot(df_filtered['Deal_Date'], df_filtered[column], label=industry)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.legend(title='Industry sector', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.title(f'{column} Over Time for Different Industry sectors')\n",
    "        plt.xlabel('Deal Date')\n",
    "        plt.ylabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plot_numerical_columns_by_industry(df_nan_completition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the two groupby**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean standard deviation for each feature\n",
    "print('With a classification group by :')\n",
    "for col in numeric_columns:\n",
    "    mean_std = df_by_classification[col].agg(['std'])\n",
    "    print(f\"Mean std for {col} : {np.round(mean_std['std'], 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean standard deviation for each feature\n",
    "print('With an Industry group by :')\n",
    "for col in numeric_columns:\n",
    "    mean_std = df_by_industrySector[col].agg(['std'])\n",
    "    print(f\"Mean std for {col} : {np.round(mean_std['std'], 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods have very similar results in terms of std, so we'll opt for the classification that has a slightly lower average standard deviation. Let's right the function to complete null values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_nan_values(df):\n",
    "\n",
    "    df_unique_isin = df.groupby('ISIN').first()\n",
    "    columns = ['Classification', 'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "               'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity', 'Year_dealdate', 'Month_dealdate']\n",
    "    df_by_classification = df_unique_isin[columns].copy()\n",
    "    df_by_classification = df_by_classification.groupby(['Classification', 'Year_dealdate']).mean().reset_index()\n",
    "\n",
    "    df_group_by_industry = df_by_classification.groupby('Classification').mean().reset_index()\n",
    "    numeric_columns = ['SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                       'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity']\n",
    "    \n",
    "    df_by_classification['additional_column'] = df_by_classification['Classification'].astype(str) + ' - ' + df_by_classification['Year_dealdate'].astype(str)\n",
    "    df['additional_column'] = df['Classification'].astype(str) + ' - ' + df['Year_dealdate'].astype(str)\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        df_by_classification[column] = df_by_classification[column].fillna(df_by_classification['Classification'].map(df_group_by_industry.set_index('Classification')[column]))\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        df[column] = df[column].fillna(df['additional_column'].map(df_by_classification.set_index('additional_column')[column]))\n",
    "\n",
    "    df.drop(columns=['additional_column'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_preprocessed = complete_nan_values(df_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the steps\n",
    "1. **Aggregation**  \n",
    "- Group initial data by ISIN, selecting the first row for each group\n",
    "- Create df_by_classification with mean values grouped by Classification and Year\n",
    "2. **Calculate Overall Means**\n",
    "- Calculate mean values for each Classification across all years in df_group_by_industry\n",
    "3. **Merge and Fill Missing Values**\n",
    "- Create a merge key in both DataFrames\n",
    "- Fill missing values in df_by_classification using overall means\n",
    "- Fill missing values in the original DataFrame (df) using grouped means\n",
    "4. **Cleanup**\n",
    "- Drop the merge key from both DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3° EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting last null values rows \n",
    "\n",
    "def delete_null_values(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.dropna(inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "df_preprocessed = delete_null_values(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique ISIN (bonds) : ', len(df_preprocessed.ISIN.unique()))\n",
    "print('Number of unique clients : ', len(df_preprocessed.company_short_name.unique()))\n",
    "print('Number of unique Industrie sectors : ', len(df_preprocessed.BloomIndustrySector.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Deal_Date analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month from 'Deal_Date'\n",
    "counts_df = df_preprocessed.groupby(['Year_dealdate', 'Month_dealdate']).size().reset_index(name='Counts')\n",
    "counts_df['Month_Year_dealdate'] = counts_df['Month_dealdate'].astype(str) + ' - ' + counts_df['Year_dealdate'].astype(str)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(x='Month_Year_dealdate', y='Counts', data=counts_df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.title('Number of Deals for Each (Year, Month) Pair\\n', fontsize=13)\n",
    "plt.xlabel('(Year, Month)')\n",
    "plt.ylabel('Number of Deals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Maturity analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maturity = df_preprocessed.dropna(subset=['maturity'], inplace=False)\n",
    "df_maturity = df_maturity.groupby('Year_maturity').size().reset_index(name='Counts')\n",
    "df_maturity.sort_values(by='Year_maturity', ascending=True)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x='Year_maturity', y='Counts', data=df_maturity)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=70, ha='right')\n",
    "plt.title('Number of deals per maturity values (annual granularity)\\n', fontsize=20)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Deals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Client (Company_short_name) analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_comp = len(df_preprocessed.company_short_name.values)\n",
    "print(\"Number of companies : \", number_of_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme du nombre de deals par company_short_name (top 20)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_20_companies = df_preprocessed['company_short_name'].value_counts().head(20)\n",
    "sns.countplot(x='company_short_name', data=df_preprocessed, order=top_20_companies.index)\n",
    "plt.title('Number of deals by company (Top 20)\\n', fontsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **B_Sides analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "palette = sns.color_palette(\"rainbow\", n_colors=2)\n",
    "sns.countplot(x='Year_dealdate', hue='B_Side', data=df_preprocessed, palette=palette)\n",
    "plt.legend(title='B_Side', labels=['0: Natixis Sells', '1: Natixis Buys'])\n",
    "plt.title('Distribution of B_Side by Year\\n', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **BloomIndustrySector analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_counts = df_preprocessed['BloomIndustrySector'].value_counts()\n",
    "colors = sns.color_palette('rainbow', len(sector_counts))\n",
    "\n",
    "# Set the threshold for displaying percentages\n",
    "percentage_threshold = 2\n",
    "def autopct_func(pct):\n",
    "    return f'{pct:.1f}%' if pct >= percentage_threshold else ''\n",
    "\n",
    "# Plot the pie chart without labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(sector_counts, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(sector_counts.index, title='Sectors', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of Values in the BloomIndustrySector Column (all RFQs)', fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unique 'ISIN' and keep the first row for each group\n",
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "\n",
    "# Count the occurrences of each sector in the new DataFrame\n",
    "sector_counts_unique_isin = df_unique_isin['BloomIndustrySector'].value_counts()\n",
    "\n",
    "# Plot the pie chart with the new DataFrame\n",
    "colors = sns.color_palette('rainbow', len(sector_counts_unique_isin))\n",
    "\n",
    "# Set the threshold for displaying percentages\n",
    "percentage_threshold = 2\n",
    "def autopct_func(pct):\n",
    "    return f'{pct:.1f}%' if pct >= percentage_threshold else ''\n",
    "\n",
    "# Plot the pie chart without labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(sector_counts_unique_isin, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(sector_counts_unique_isin.index, title='Sectors', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of Values in the BloomIndustrySector Column (Unique ISIN)', fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_industry = df_preprocessed[['ISIN', 'BloomIndustrySector']].copy()\n",
    "df_count_trades = df_industry.groupby('ISIN').agg(Count=('BloomIndustrySector', 'count'), First_BloomIndustrySector=('BloomIndustrySector', 'first')).reset_index()\n",
    "df_avg_trades_per_group = df_count_trades.groupby('First_BloomIndustrySector')['Count'].mean().reset_index()\n",
    "df_avg_trades_per_group.sort_values(by='Count', ascending=False, inplace=True)\n",
    "\n",
    "# Plot the bar chart for average number of trades per BloomIndustrySector\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.barplot(x='First_BloomIndustrySector', y='Count', data=df_avg_trades_per_group, palette='rainbow')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.title('Average Number of Trades per Bloomberg Industry Sector\\n (unique ISIN)', fontsize=15)\n",
    "plt.xlabel('BloomIndustrySector')\n",
    "plt.ylabel('Average Number of Trades')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at sector\n",
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "sector_counts_unique_isin = df_unique_isin['BloomIndustrySector'].value_counts()\n",
    "sector_counts_unique_isin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Industry group\n",
    "number_industries = len(df_unique_isin['BloomIndustryGroup'].unique())\n",
    "print('Number of industries : ', number_industries)\n",
    "df_unique_isin.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_financial = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Financial']\n",
    "df_financial.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_government = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Government']\n",
    "df_government.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utilities = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Utilities']\n",
    "df_utilities.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_industrial = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Industrial']\n",
    "df_industrial.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_communication = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Communications']\n",
    "df_communication.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Energy']\n",
    "df_energy.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_technology = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Technology']\n",
    "df_technology.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic_materials = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Basic Materials']\n",
    "df_basic_materials.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diversified = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Diversified']\n",
    "df_diversified.BloomIndustryGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Industry subgroup\n",
    "number_subindustries = len(df_unique_isin['BloomIndustrySubGroup'].unique())\n",
    "print('Number of sub industries : ', number_subindustries)\n",
    "df_unique_isin.BloomIndustrySubGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_financial = df_unique_isin[df_unique_isin['BloomIndustryGroup']=='Banks']\n",
    "df_financial.BloomIndustrySubGroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_government = df_unique_isin[df_unique_isin['BloomIndustryGroup']=='Sovereign']\n",
    "df_government.BloomIndustrySubGroup.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Days_to_Maturity analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "df_unique_isin.Days_to_Maturity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the number of deals per company_short_name (top 20) with bins of size 50\n",
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_unique_isin['Days_to_Maturity'], bins=range(0, max(df_preprocessed['Days_to_Maturity']) + 1000, 1000))\n",
    "plt.title('Days to maturity\\n', fontsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Fitch Credit rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unique 'ISIN' and keep the first row for each group\n",
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "\n",
    "# Count the occurrences of each sector in the new DataFrame\n",
    "sector_counts_unique_isin = df_unique_isin['Rating_Fitch'].value_counts()\n",
    "\n",
    "# Plot the pie chart with the new DataFrame\n",
    "colors = sns.color_palette('rainbow', len(sector_counts_unique_isin))\n",
    "\n",
    "# Set the threshold for displaying percentages\n",
    "percentage_threshold = 2\n",
    "def autopct_func(pct):\n",
    "    return f'{pct:.1f}%' if pct >= percentage_threshold else ''\n",
    "\n",
    "# Plot the pie chart without labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(sector_counts_unique_isin, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(sector_counts_unique_isin.index, title='Fitch Rating', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of Fitch ratings among all data (Unique ISIN)', fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unique 'ISIN' and keep the first row for each group\n",
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "\n",
    "df_government = df_unique_isin[df_unique_isin['BloomIndustrySector']=='Government'].copy()\n",
    "\n",
    "# Count the occurrences of each sector in the new DataFrame\n",
    "sector_counts_unique_isin = df_government['Rating_Fitch'].value_counts()\n",
    "\n",
    "# Plot the pie chart with the new DataFrame\n",
    "colors = sns.color_palette('rainbow', len(sector_counts_unique_isin))\n",
    "\n",
    "# Set the threshold for displaying percentages\n",
    "percentage_threshold = 2\n",
    "def autopct_func(pct):\n",
    "    return f'{pct:.1f}%' if pct >= percentage_threshold else ''\n",
    "\n",
    "# Plot the pie chart without labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(sector_counts_unique_isin, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(sector_counts_unique_isin.index, title='Fitch Rating', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of Fitch ratings among data government (Unique ISIN)', fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "36.6+18.4+7.3+6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the common attributes\n",
    "colors = sns.color_palette('rainbow')\n",
    "percentage_threshold = 2\n",
    "autopct_func = lambda pct: f'{pct:.1f}%' if pct >= percentage_threshold else ''\n",
    "\n",
    "# Plot the pie chart for the entire dataset\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "Fitch_counts_all = df_preprocessed['Rating_Fitch'].value_counts()\n",
    "plt.pie(Fitch_counts_all, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(Fitch_counts_all.index, title='Fitch rating', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of Fitch ratings among all data', fontsize=15)\n",
    "\n",
    "# Plot the pie chart for the government data\n",
    "plt.subplot(1, 2, 2)\n",
    "df_government = df_preprocessed[df_preprocessed['BloomIndustrySector']=='Government'].copy()\n",
    "Fitch_counts_government = df_government['Rating_Fitch'].value_counts()\n",
    "plt.pie(Fitch_counts_government, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(Fitch_counts_government.index, title='Fitch rating', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of Fitch ratings among government data', fontsize=15)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **SP Credit rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the common attributes\n",
    "colors = sns.color_palette('rainbow')\n",
    "percentage_threshold = 2\n",
    "autopct_func = lambda pct: f'{pct:.1f}%' if pct >= percentage_threshold else ''\n",
    "\n",
    "# Plot the pie chart for the entire dataset\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "moodys_counts_all = df_preprocessed['Rating_SP'].value_counts()\n",
    "plt.pie(moodys_counts_all, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(moodys_counts_all.index, title='SP rating', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of SP ratings among all data', fontsize=15)\n",
    "\n",
    "# Plot the pie chart for the government data\n",
    "plt.subplot(1, 2, 2)\n",
    "df_government = df_preprocessed[df_preprocessed['BloomIndustrySector']=='Government'].copy()\n",
    "moodys_counts_all = df_government['Rating_SP'].value_counts()\n",
    "plt.pie(moodys_counts_all, labels=None, autopct=autopct_func, startangle=140, colors=colors)\n",
    "plt.legend(moodys_counts_all.index, title='SP rating', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "plt.title('Distribution of SP ratings among government data', fontsize=15)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Currency analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each currency\n",
    "ccy_counts = df_preprocessed['Ccy'].value_counts()\n",
    "print(ccy_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each currency\n",
    "ccy_counts = df_preprocessed['Ccy'].value_counts()\n",
    "ccy_percentage = ccy_counts / len(df_preprocessed) * 100\n",
    "print(ccy_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The great majority of bonds (99.5%) are in euros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Bond Price analysis (B_price)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unique 'ISIN' and keep the first row for each group\n",
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "\n",
    "df_EUR = df_unique_isin[df_unique_isin['Ccy']=='EUR']\n",
    "df_EUR['B_Price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "df_EUR['B_Price'].hist(bins=30, grid=False, density=True)\n",
    "plt.title('Histogram of B_Price, only for EUR currency')\n",
    "plt.xlabel('B_Price')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Country analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_isin = df_preprocessed.groupby('ISIN').first()\n",
    "country_counts = df_unique_isin['Country'].value_counts()\n",
    "\n",
    "# Plot the bar chart for country columns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=country_counts.index, y=country_counts, palette='rainbow')\n",
    "plt.title('Distribution of values in the Country column\\n', fontsize = 15)\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage distribution\n",
    "country_percentage = (country_counts / country_counts.sum()) * 100\n",
    "\n",
    "# Plot the bar chart for country columns with percentages\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=country_percentage.index, y=country_percentage, palette='rainbow')\n",
    "plt.title('Distribution of values in the Country column (Percentage)\\n', fontsize = 15)\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_percentage.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority are Italian, French and German bonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Coupon analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "df_preprocessed['Coupon'].hist(bins=30, grid=False, density=True)\n",
    "plt.title('Histogram of Coupon values')\n",
    "plt.xlabel('Coupon')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.GSpreadMid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4° Exploring the heterogeneity of clients and bonds\n",
    "\n",
    "- **Exploring the heterogeneity of clients** (p.13 of the thesis): most active client ? What is he accounting for among all performed RFQs ? Find something like '80% of the accounted RFQs were made by less than 10% of the client pool'. Plot the graph Cumulative propoertion of clients as a function of cumulative proportion of RFQs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataframe\n",
    "df_clients = df_preprocessed[['ISIN', 'company_short_name']]\n",
    "df_RFQ_count = df_clients.groupby(by='company_short_name').agg(count=('company_short_name','count')).reset_index()\n",
    "df_RFQ_count.columns = ['company_short_name', 'RFQ_count']\n",
    "total_RFQ_transactions = df_RFQ_count.RFQ_count.sum()\n",
    "df_RFQ_count['RFQ_count_perc'] = df_RFQ_count['RFQ_count']*100/total_RFQ_transactions\n",
    "\n",
    "df_RFQ_count.sort_values(by='RFQ_count', ascending=False, inplace=True)\n",
    "df_RFQ_count['client_number'] = 1\n",
    "df_RFQ_count['cum_sum_client'] = np.round(df_RFQ_count['client_number'].cumsum() * 100 /(df_RFQ_count.shape[0]), 2)\n",
    "df_RFQ_count.drop(columns=['client_number'], inplace=True)\n",
    "df_RFQ_count['cum_sum_RFQ'] = np.round(df_RFQ_count['RFQ_count_perc'].cumsum(), 2)\n",
    "\n",
    "# Generate the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_RFQ_count['cum_sum_RFQ'], df_RFQ_count['cum_sum_client'], linestyle='-')\n",
    "plt.title('Cumulative Proportion of Clients vs Cumulative Proportion of RFQs\\n', fontsize=15)\n",
    "plt.xlabel('Cumulative Proportion of RFQs (%)')\n",
    "plt.ylabel('Cumulative Proportion of Clients (%)')\n",
    "plt.axvline(x=76, color='red', linestyle='--') \n",
    "plt.axhline(y=20, color='red', linestyle='--') \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFQ_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFQ_count[df_RFQ_count['cum_sum_client']>20].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations regarding the heterogeneity of clients : \n",
    "- The **most active client** (societe generale mer) accounted for 27.95% of all performed RFQs alone \n",
    "- **76% of the accounted RFQs** were made by less of **20% of the client pool**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring the heterogeneity of bonds**  (p.13 of the thesis): % of bonds that are shared among several clients ? Find somthing like '80% of the RFQs were done on more than 30% of the considered assets.' Plot the cumulative number of assets as a function of the cumulative proportion of RFQs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bonds = df_preprocessed[['ISIN', 'cdcissuer']].copy()\n",
    "df_bonds = df_bonds.groupby(by='ISIN').agg('count').reset_index()\n",
    "df_bonds.columns= ['ISIN', 'count']\n",
    "df_bonds.sort_values(by='count', ascending=False, inplace=True)\n",
    "total_bond = df_preprocessed.shape[0]\n",
    "df_bonds['count_perc'] = np.round(df_bonds['count']*100/total_bond, 2)\n",
    "df_bonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bonds = df_preprocessed[['ISIN', 'cdcissuer']].copy()\n",
    "df_bonds = df_bonds.groupby(by='cdcissuer').agg('count').reset_index()\n",
    "df_bonds.columns = ['cdcissuer', 'count']\n",
    "df_bonds.sort_values(by='count', ascending=False, inplace=True)\n",
    "total_bond = df_preprocessed.shape[0]\n",
    "df_bonds['count_perc'] = np.round(df_bonds['count']*100/total_bond, 2)\n",
    "df_bonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bonds = df_preprocessed[['ISIN', 'cdcissuer']].copy()\n",
    "df_bonds = df_bonds.groupby(by='ISIN').agg('count').reset_index()\n",
    "df_bonds.columns= ['ISIN', 'count']\n",
    "df_bonds.sort_values(by='count', ascending=False, inplace=True)\n",
    "total_bond = df_preprocessed.shape[0]\n",
    "df_bonds['count_perc'] = np.round(df_bonds['count']*100/total_bond, 2)\n",
    "df_bonds['count_perc_cumsum'] = df_bonds['count_perc'].cumsum()\n",
    "\n",
    "df_bonds['number_asset'] = 1\n",
    "total_asset = df_bonds.shape[0]\n",
    "df_bonds['cum_sum_asset'] = np.round(df_bonds['number_asset'].cumsum()*100/total_asset, 2)\n",
    "df_bonds.drop(columns=['number_asset', 'count'], inplace=True)\n",
    "\n",
    "print('First line where the cumulated sum of RFQs is higher than 80% :')\n",
    "df_higher_20 = df_bonds[df_bonds['count_perc_cumsum']>80]\n",
    "print(df_higher_20.iloc[0])\n",
    "\n",
    "# Plot the histogram \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_bonds['count_perc_cumsum'], df_bonds['cum_sum_asset'], linestyle='-')\n",
    "plt.title('Cumulative Proportion of Assets vs Cumulative Proportion of RFQs\\n', fontsize=15)\n",
    "plt.xlabel('Cumulative Proportion of RFQs (%)')\n",
    "plt.ylabel('Cumulative Proportion of Assets (%)')\n",
    "plt.axvline(x=80.01, color='red', linestyle='--') \n",
    "plt.axhline(y=5.23, color='red', linestyle='--') \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations regarding the heterogeneity of bonds :\n",
    "- The **most active bond** (ISIN : IT0003934657) accounted for 2.65% of the all RFQs\n",
    "- The **most active bond issuer** (cdcissuer : BTP) accounted for 50% of all RFQs\n",
    "- **80% of the RFQs** were done on a bit more than **5% of the considered assets**. The heterogeity of bonds is then well more pronounced that the heterogeity of clients.\n",
    "\n",
    "**Conclusion :** Most clients only trade a handful of bonds, and most bonds are only traded by a handful of clients. This problemn is referred in the page 13/14 of the thesis as the **long tail problem** :\n",
    "\n",
    "\"It can consequently become diffcult to build recommender systems able to propose the entire asset pool to a given client, and reciprocally. This phenomenon is referred to in the literature as the long tail problem (Park and Tuzhilin, 2008), which refers to the fact that for most items, very few activity is recorded. Park and Tuzhilin (2008) proposes a way to tackle this by splitting the item pool in a head and tail parts and train models on each of these parts, the tail model being trained on clustered items. More broadly, the long tail is tackled by systems that favor diverse recommendations, i.e., recommendations that are made outside of the usual clients’ patterns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5° Writing EDA functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bond Analysis functions (given ISIN code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. General Info display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def plot_double_general_info(df, initial_ISIN, df_similar_ISIN, probabilities):\n",
    "    \n",
    "    # Filter DataFrame for initial ISIN\n",
    "    df_initial_ISIN = df[df['ISIN'] == initial_ISIN].copy()\n",
    "    df_initial_ISIN.sort_values(by='Deal_Date', inplace=True)\n",
    "\n",
    "    # Check if the DataFrame is empty after filtering\n",
    "    if df_initial_ISIN.empty:\n",
    "        return \"No data found for the given initial ISIN.\"\n",
    "    \n",
    "    # Format the maturity date for initial ISIN\n",
    "    maturity_date_initial = df_initial_ISIN.maturity.iloc[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    maturity_date_initial = maturity_date_initial[:-9]  \n",
    "\n",
    "    # Create a list of lists with data for initial ISIN\n",
    "    data = [\n",
    "        [str(df_initial_ISIN.ISIN.iloc[0]) + ' (Initial)',\n",
    "         df_initial_ISIN.Country.iloc[0],\n",
    "         df_initial_ISIN.BloomIndustrySector.iloc[0],\n",
    "         df_initial_ISIN.BloomIndustrySubGroup.iloc[0],\n",
    "         maturity_date_initial,\n",
    "         np.round(df_initial_ISIN.Coupon.iloc[0], 2),\n",
    "         df_initial_ISIN.Frequency.iloc[0],\n",
    "         100  # Proximity value for the initial ISIN\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Iterate through similar ISINs and probabilities\n",
    "    for similar_ISIN, proximity in zip(df_similar_ISIN, probabilities):\n",
    "        # Filter DataFrame for similar ISIN\n",
    "        df_similar_ISIN = df[df['ISIN'] == similar_ISIN].copy()\n",
    "        df_similar_ISIN.sort_values(by='Deal_Date', inplace=True)\n",
    "\n",
    "        # Check if the DataFrame is empty after filtering\n",
    "        if df_similar_ISIN.empty:\n",
    "            continue\n",
    "\n",
    "        # Format the maturity date for similar ISIN\n",
    "        maturity_date_similar = df_similar_ISIN.maturity.iloc[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        maturity_date_similar = maturity_date_similar[:-9]  \n",
    "\n",
    "        # Add data for similar ISIN to the list\n",
    "        data.append([df_similar_ISIN.ISIN.iloc[0],\n",
    "                     df_similar_ISIN.Country.iloc[0],\n",
    "                     df_similar_ISIN.BloomIndustrySector.iloc[0],\n",
    "                     df_similar_ISIN.BloomIndustrySubGroup.iloc[0],\n",
    "                     maturity_date_similar,\n",
    "                     np.round(df_similar_ISIN.Coupon.iloc[0], 2),\n",
    "                     df_similar_ISIN.Frequency.iloc[0],\n",
    "                     np.round(100* proximity, 2)  # Proximity value for the similar ISIN\n",
    "                    ])\n",
    "\n",
    "    # Create a fancy grid table using tabulate\n",
    "    table = tabulate(data, headers=[\"ISIN code\", \"Country\", \"Sector\", \"Industry Subgroup\", \n",
    "                                    \"Maturity\", \"Coupon\", \"Frequency\", \"Proximity (in %)\"],\n",
    "                     tablefmt='fancy_grid', numalign=\"center\", stralign=\"center\", colalign=(\"center\",),\n",
    "                     showindex=False)\n",
    "\n",
    "    # Print the table\n",
    "    print(table)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "recommended_ISIN = ['XS1991125896','XS1998025008', 'XS2004880832', 'XS1989375412', 'XS2031862076']\n",
    "ISIN_ex1 = 'XS1991125896'\n",
    "ISIN_ex2 = 'XS1998025008'\n",
    "ISIN_ex3 = 'XS2004880832'\n",
    "ISIN_ex4 = 'XS1989375412'\n",
    "ISIN_ex5 = 'XS2031862076'\n",
    "probabilities = [0.99466381, 0.99306537, 0.99163003, 0.99149678, 0.99149156]\n",
    "\n",
    "initial_ISIN = 'XS1985806600'\n",
    "\n",
    "plot_double_general_info(df_preprocessed, initial_ISIN, recommended_ISIN, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def plot_double_general_info(df, ISIN1, ISIN2):\n",
    "    \n",
    "    # Filter DataFrame for ISIN1\n",
    "    df_ISIN1 = df[df['ISIN'] == ISIN1].copy()\n",
    "    df_ISIN1.sort_values(by='Deal_Date', inplace=True)\n",
    "    \n",
    "    # Filter DataFrame for ISIN2\n",
    "    df_ISIN2 = df[df['ISIN'] == ISIN2].copy()\n",
    "    df_ISIN2.sort_values(by='Deal_Date', inplace=True)\n",
    "\n",
    "    # Check if the DataFrames are empty after filtering\n",
    "    if df_ISIN1.empty or df_ISIN2.empty:\n",
    "        return \"No data found for the given ISINs.\"\n",
    "    \n",
    "    # Format the maturity date for ISIN1\n",
    "    maturity_date1 = df_ISIN1.maturity.iloc[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    maturity_date1 = maturity_date1[:-9]  \n",
    "    price1 = df_ISIN1['B_Price'].iloc[-1]\n",
    "\n",
    "    # Format the maturity date for ISIN2\n",
    "    maturity_date2 = df_ISIN2.maturity.iloc[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    maturity_date2 = maturity_date2[:-9]  \n",
    "    price2 = df_ISIN2['B_Price'].iloc[-1]\n",
    "\n",
    "    # Create a list of lists with data for both ISINs\n",
    "    data = [\n",
    "        [str(df_ISIN1.ISIN.iloc[0]) + ' (Initial)',\n",
    "         df_ISIN1.cdcissuer.iloc[0],\n",
    "         df_ISIN1.Country.iloc[0],\n",
    "         df_ISIN1.BloomIndustrySector.iloc[0],\n",
    "         df_ISIN1.BloomIndustrySubGroup.iloc[0],\n",
    "         maturity_date1,\n",
    "         price1,\n",
    "         np.round(df_ISIN1.Coupon.iloc[0], 2),\n",
    "         df_ISIN1.Frequency.iloc[0],\n",
    "        ],\n",
    "        \n",
    "        [df_ISIN2.ISIN.iloc[0]+ ' (Recommended)',\n",
    "         df_ISIN2.cdcissuer.iloc[0],\n",
    "         df_ISIN2.Country.iloc[0],\n",
    "         df_ISIN2.BloomIndustrySector.iloc[0],\n",
    "         df_ISIN2.BloomIndustrySubGroup.iloc[0],\n",
    "         maturity_date2,\n",
    "         price2,\n",
    "         np.round(df_ISIN2.Coupon.iloc[0], 2),\n",
    "         df_ISIN2.Frequency.iloc[0],\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Create a fancy grid table using tabulate\n",
    "    table = tabulate(data, headers=[\"ISIN code\", \"Issuer\", \"Country\", \"Sector\", \"Industry Subgroup\", \n",
    "                                    \"Maturity\", \"Price (EUR)\", \"Coupon\", \"Frequency\"],\n",
    "                     tablefmt='fancy_grid', numalign=\"center\", stralign=\"center\", colalign=(\"center\",),\n",
    "                     showindex=False)\n",
    "\n",
    "    # Print the table\n",
    "    print(table)\n",
    "\n",
    "# Example usage\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "plot_double_general_info(df_preprocessed, initial_ISIN, recommended_ISIN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. **Bond Price evolution** as a function of the deal date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_price_evolution(df, ISIN1, ISIN2):\n",
    "    \"\"\"\n",
    "    Plots the price evolution of a bond over its deal dates.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing bond data.\n",
    "    - ISIN (str): The International Securities Identification Number of the bond.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Process the dataframe\n",
    "    df_ISIN1 = df[df['ISIN'] == ISIN1].copy()\n",
    "    df_ISIN1.sort_values(by='Deal_Date', inplace=True)\n",
    "\n",
    "    df_ISIN2 = df[df['ISIN'] == ISIN2].copy()\n",
    "    df_ISIN2.sort_values(by='Deal_Date', inplace=True)\n",
    "\n",
    "    # Plot the figure with violet color scheme\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    df_ISIN1_currency = df_ISIN1.iloc[0].Ccy\n",
    "    df_ISIN2_currency = df_ISIN2.iloc[0].Ccy\n",
    "    plt.plot(df_ISIN1['Deal_Date'], df_ISIN1['B_Price'], color='purple', linestyle='-', linewidth=2, marker='o', markersize=4, label=f'Initial ISIN {ISIN1} (in {df_ISIN1_currency})')\n",
    "    plt.plot(df_ISIN2['Deal_Date'], df_ISIN2['B_Price'], color='purple', alpha=0.3, linestyle='-', marker='o',  markersize=4, label=f'Recommended ISIN {ISIN2} (in {df_ISIN2_currency})')\n",
    "    plt.xlabel('Deal Date', fontsize=15)\n",
    "    plt.ylabel(f'Bond Price', fontsize=15)\n",
    "    plt.title(f'Bond Price evolution as a function of the Deal Date\\n', fontsize=18)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7) \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "plot_double_price_evolution(df_preprocessed, initial_ISIN, recommended_ISIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Display of the grades comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triple_vertical_gauges(df, ISIN):\n",
    "    # Dataframe for the specified ISIN\n",
    "    df_ISIN = df[df['ISIN'] == ISIN]\n",
    "\n",
    "    sorted_fitch_ratings = ['WD', 'B+', 'BB-', 'BB', 'BB+', 'BBB-', 'BBB', 'BBB+', 'A-', 'A', 'A+', 'AA', 'AA+', 'AAA']\n",
    "    sorted_moodys_ratings = ['NR', 'WR', 'Caa1', 'B3', 'B2', 'B1', 'Ba3', 'Ba2', 'Ba1', 'Baa3', 'Baa2', 'Baa1', 'A3', 'A2', 'A1', 'Aa3', 'Aa2', 'Aa1', 'Aaa']\n",
    "    sorted_sp_ratings = ['NR', 'CCC+', 'CC', 'B+', 'BB', 'BB+', 'BB-', 'BBB-', 'BBB', 'BBB+', 'A-', 'A', 'A+', 'AA-', 'AA', 'AA+', 'AAA']\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 4))\n",
    "\n",
    "    # Plot for Fitch\n",
    "    cmap_fitch = plt.get_cmap('PRGn')\n",
    "    colors_fitch = [cmap_fitch(i / len(sorted_fitch_ratings)) for i in range(len(sorted_fitch_ratings))]\n",
    "    grade_fitch = df_ISIN['Rating_Fitch'].iloc[0]\n",
    "\n",
    "    for i, rating in enumerate(sorted_fitch_ratings):\n",
    "        ax1.axvspan(i, i + 1, facecolor=colors_fitch[i], alpha=0.8)\n",
    "\n",
    "    if grade_fitch and grade_fitch in sorted_fitch_ratings:\n",
    "        grade_index_fitch = sorted_fitch_ratings.index(grade_fitch)\n",
    "        ax1.axvline(x=grade_index_fitch + 0.5, color='red', linestyle='-', linewidth=3)\n",
    "\n",
    "    ax1.set_xticks(np.arange(len(sorted_fitch_ratings)) + 0.5)\n",
    "    ax1.set_xticklabels(sorted_fitch_ratings)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_aspect('auto')\n",
    "    ax1.set_title(f'Fitch Ratings')\n",
    "\n",
    "    # Plot for Moodys\n",
    "    cmap_moodys = plt.get_cmap('PRGn')\n",
    "    colors_moodys = [cmap_moodys(i / len(sorted_moodys_ratings)) for i in range(len(sorted_moodys_ratings))]\n",
    "    grade_moodys = df_ISIN['Rating_Moodys'].iloc(0)\n",
    "\n",
    "    for i, rating in enumerate(sorted_moodys_ratings):\n",
    "        ax2.axvspan(i, i + 1, facecolor=colors_moodys[i], alpha=0.8)\n",
    "\n",
    "    if grade_moodys and grade_moodys[0] in sorted_moodys_ratings:\n",
    "        grade_index_moodys = sorted_moodys_ratings.index(grade_moodys[0])\n",
    "        ax2.axvline(x=grade_index_moodys + 0.5, color='red', linestyle='-', linewidth=3)\n",
    "\n",
    "    ax2.set_xticks(np.arange(len(sorted_moodys_ratings)) + 0.5)\n",
    "    ax2.set_xticklabels(sorted_moodys_ratings)\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_aspect('auto')\n",
    "    ax2.set_title(f'Moodys Ratings')\n",
    "\n",
    "    # Plot for SP\n",
    "    cmap_sp = plt.get_cmap('PRGn')\n",
    "    colors_sp = [cmap_sp(i / len(sorted_sp_ratings)) for i in range(len(sorted_sp_ratings))]\n",
    "    grade_sp = df_ISIN['Rating_SP'].iloc[0]\n",
    "\n",
    "    for i, rating in enumerate(sorted_sp_ratings):\n",
    "        ax3.axvspan(i, i + 1, facecolor=colors_sp[i], alpha=0.8)\n",
    "\n",
    "    if grade_sp and grade_sp in sorted_sp_ratings:\n",
    "        grade_index_sp = sorted_sp_ratings.index(grade_sp)\n",
    "        ax3.axvline(x=grade_index_sp + 0.5, color='red', linestyle='-', linewidth=3)\n",
    "\n",
    "    ax3.set_xticks(np.arange(len(sorted_sp_ratings)) + 0.5)\n",
    "    ax3.set_xticklabels(sorted_sp_ratings)\n",
    "    ax3.set_yticks([])\n",
    "    ax3.set_aspect('auto')\n",
    "    ax3.set_title(f'SP Ratings')\n",
    "\n",
    "    plt.suptitle(f'Rating of ISIN {ISIN}', fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run\n",
    "initial_ISIN = 'XS1985806600'\n",
    "plot_triple_vertical_gauges(df_preprocessed, initial_ISIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_horizontal_gauges(df, ISIN1, ISIN2, rating_name='Fitch'):\n",
    "\n",
    "    # Dataframes for both ISINs\n",
    "    df_ISIN1 = df[df['ISIN'] == ISIN1]\n",
    "    df_ISIN2 = df[df['ISIN'] == ISIN2]\n",
    "\n",
    "    if rating_name == 'Fitch':\n",
    "        sorted_ratings = ['WD', 'B+', 'BB-', 'BB', 'BB+', 'BBB-', 'BBB', 'BBB+', 'A-', 'A', 'A+', 'AA', 'AA+', 'AAA']\n",
    "        grade1 = df_ISIN1.Rating_Fitch.iloc[0]\n",
    "        grade2 = df_ISIN2.Rating_Fitch.iloc[0]\n",
    "\n",
    "    if rating_name == 'Moodys':\n",
    "        sorted_ratings = ['NR', 'WR', 'Caa1', 'B3', 'B2', 'B1', 'Ba3', 'Ba2', 'Ba1', 'Baa3', 'Baa2', 'Baa1', 'A3', 'A2', 'A1', 'Aa3', 'Aa2', 'Aa1', 'Aaa']\n",
    "        grade1 = df_ISIN1.Rating_Moodys.iloc[0]\n",
    "        grade2 = df_ISIN2.Rating_Moodys.iloc[0]\n",
    "\n",
    "    if rating_name == 'SP':\n",
    "        sorted_ratings = ['NR', 'CCC+', 'CC', 'B+', 'BB', 'BB+', 'BB-', 'BBB-', 'BBB', 'BBB+', 'A-', 'A', 'A+', 'AA-', 'AA', 'AA+', 'AAA']\n",
    "        grade1 = df_ISIN1.Rating_SP.iloc[0]\n",
    "        grade2 = df_ISIN2.Rating_SP.iloc[0]\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 2.1))\n",
    "\n",
    "    # Plot for ISIN1\n",
    "    cmap = plt.get_cmap('PRGn')  # Modified colormap for red to green gradient\n",
    "    colors1 = [cmap(i / len(sorted_ratings)) for i in range(len(sorted_ratings))]\n",
    "    for i, rating in enumerate(sorted_ratings):\n",
    "        ax1.axvspan(i, i + 1, facecolor=colors1[i], alpha=0.8)\n",
    "\n",
    "    if grade1 and grade1 in sorted_ratings:\n",
    "        grade_index1 = sorted_ratings.index(grade1)\n",
    "        ax1.axvline(x=grade_index1 + 0.5, color='red', linestyle='-', linewidth=3)\n",
    "\n",
    "    ax1.set_xticks(np.arange(len(sorted_ratings)) + 0.5)\n",
    "    ax1.set_xticklabels(sorted_ratings, fontsize=9)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_aspect('auto')\n",
    "    ax1.set_title(f'{rating_name} rating of initial ISIN ({ISIN1})')\n",
    "\n",
    "    # Plot for ISIN2\n",
    "    colors2 = [cmap(i / len(sorted_ratings)) for i in range(len(sorted_ratings))]\n",
    "    for i, rating in enumerate(sorted_ratings):\n",
    "        ax2.axvspan(i, i + 1, facecolor=colors2[i], alpha=0.8)\n",
    "\n",
    "    if grade2 and grade2 in sorted_ratings:\n",
    "        grade_index2 = sorted_ratings.index(grade2)\n",
    "        ax2.axvline(x=grade_index2 + 0.5, color='red', linestyle='-', linewidth=3)\n",
    "\n",
    "    ax2.set_xticks(np.arange(len(sorted_ratings)) + 0.5)\n",
    "    ax2.set_xticklabels(sorted_ratings, fontsize=9)\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_aspect('auto')\n",
    "    ax2.set_title(f'{rating_name} rating of recommended ISIN ({ISIN2})')\n",
    "    #plt.suptitle(f'Comparison of {rating_name} Ratings of the two ISINs', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "\n",
    "# Run\n",
    "plot_double_horizontal_gauges(df_preprocessed, initial_ISIN, recommended_ISIN, rating_name='Fitch')\n",
    "plot_double_horizontal_gauges(df_preprocessed, initial_ISIN, recommended_ISIN, rating_name='Moodys')\n",
    "plot_double_horizontal_gauges(df_preprocessed, initial_ISIN, recommended_ISIN, rating_name='SP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Display the clients repartition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clients_repartition(df_preprocessed, ISIN_ex1):\n",
    "\n",
    "    # Generate the dataframe\n",
    "    df_ISIN = df_preprocessed[df_preprocessed['ISIN'] == ISIN_ex1].copy()\n",
    "    clien_unique_count = df_ISIN['company_short_name'].value_counts()\n",
    "    client_count_df = pd.DataFrame(clien_unique_count.reset_index())\n",
    "    total_count = client_count_df['count'].sum()\n",
    "    client_count_df['count_perc'] = np.round(client_count_df['count'] * 100 / total_count, 2)\n",
    "    client_count_df.drop(columns = ['count'], inplace=True)\n",
    "    df_client_other = client_count_df[client_count_df['count_perc'] < 6].copy()\n",
    "    client_count_df = client_count_df[client_count_df['count_perc'] >= 6].copy()\n",
    "    count_perc_other = np.round(df_client_other.count_perc.sum(), 2)\n",
    "\n",
    "    # Add the 'Other' line to the DataFrame\n",
    "    line_to_add = pd.DataFrame({'company_short_name': ['Others'], 'count_perc': [count_perc_other]})\n",
    "    client_count_df = pd.concat([client_count_df, line_to_add], ignore_index=True, axis=0)\n",
    "\n",
    "    # Plotting a pie chart using the rainbow colormap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = sns.color_palette(\"rainbow\", len(client_count_df))\n",
    "    plt.pie(client_count_df['count_perc'], autopct='%1.1f%%', colors=colors)\n",
    "    plt.title('Distribution of Client Transactions', fontsize=15)\n",
    "\n",
    "    # Add legend at the top right\n",
    "    plt.legend(client_count_df['company_short_name'], bbox_to_anchor=(1, 1), loc='upper left', title='Company Names')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "initial_ISIN = 'XS1985806600'\n",
    "plot_clients_repartition(df_preprocessed, initial_ISIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_clients_repartition(df_preprocessed, ISIN_ex1, ISIN_ex2):\n",
    "    # Generate the dataframe for ISIN_ex1\n",
    "    df_ISIN1 = df_preprocessed[df_preprocessed['ISIN'] == ISIN_ex1].copy()\n",
    "    clien_unique_count1 = df_ISIN1['company_short_name'].value_counts()\n",
    "    client_count_df1 = pd.DataFrame(clien_unique_count1.reset_index())\n",
    "    total_count1 = client_count_df1['count'].sum()\n",
    "    client_count_df1['count_perc'] = np.round(client_count_df1['count'] * 100 / total_count1, 2)\n",
    "    client_count_df1.drop(columns=['count'], inplace=True)\n",
    "    df_client_other1 = client_count_df1[client_count_df1['count_perc'] < 4].copy()\n",
    "    client_count_df1 = client_count_df1[client_count_df1['count_perc'] >= 4].copy()\n",
    "    count_perc_other1 = np.round(df_client_other1.count_perc.sum(), 2)\n",
    "\n",
    "    # Add the 'Other' line to the DataFrame for ISIN_ex1\n",
    "    line_to_add1 = pd.DataFrame({'company_short_name': ['Others'], 'count_perc': [count_perc_other1]})\n",
    "    client_count_df1 = pd.concat([client_count_df1, line_to_add1], ignore_index=True, axis=0)\n",
    "\n",
    "    # Generate the dataframe for ISIN_ex2\n",
    "    df_ISIN2 = df_preprocessed[df_preprocessed['ISIN'] == ISIN_ex2].copy()\n",
    "    clien_unique_count2 = df_ISIN2['company_short_name'].value_counts()\n",
    "    client_count_df2 = pd.DataFrame(clien_unique_count2.reset_index())\n",
    "    total_count2 = client_count_df2['count'].sum()\n",
    "    client_count_df2['count_perc'] = np.round(client_count_df2['count'] * 100 / total_count2, 2)\n",
    "    client_count_df2.drop(columns=['count'], inplace=True)\n",
    "    df_client_other2 = client_count_df2[client_count_df2['count_perc'] < 6].copy()\n",
    "    client_count_df2 = client_count_df2[client_count_df2['count_perc'] >= 6].copy()\n",
    "    count_perc_other2 = np.round(df_client_other2.count_perc.sum(), 2)\n",
    "\n",
    "    # Add the 'Other' line to the DataFrame for ISIN_ex2\n",
    "    line_to_add2 = pd.DataFrame({'company_short_name': ['Others'], 'count_perc': [count_perc_other2]})\n",
    "    client_count_df2 = pd.concat([client_count_df2, line_to_add2], ignore_index=True, axis=0)\n",
    "\n",
    "    # Plotting pie charts side by side\n",
    "    plt.figure(figsize=(20, 6))\n",
    "\n",
    "    # Plot for ISIN_ex1\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors1 = sns.color_palette(\"rainbow\", len(client_count_df1))\n",
    "    plt.pie(client_count_df1['count_perc'], autopct='%1.1f%%', colors=colors1)\n",
    "    plt.title(f'Distribution of Clients for Initial ISIN ({ISIN_ex1})', fontsize=18)\n",
    "    plt.legend(client_count_df1['company_short_name'], bbox_to_anchor=(0.95, 0.95), \n",
    "               loc='upper left', title='Company Names')\n",
    "\n",
    "    # Plot for ISIN_ex2\n",
    "    plt.subplot(1, 2, 2)\n",
    "    colors2 = sns.color_palette(\"rainbow\", len(client_count_df2))\n",
    "    plt.pie(client_count_df2['count_perc'], autopct='%1.1f%%', colors=colors2)\n",
    "    plt.title(f'Distribution of Clients for Recommended ISIN ({ISIN_ex2})', fontsize=18)\n",
    "    plt.legend(client_count_df2['company_short_name'], bbox_to_anchor=(0.95, 0.95), \n",
    "               loc='upper left', title='Company Names')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "\n",
    "# Example usage:\n",
    "plot_double_clients_repartition(df_preprocessed, initial_ISIN, recommended_ISIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed[df_preprocessed['ISIN'] == initial_ISIN].cdcissuer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F. Other numerical values display as a function of the deal date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_GSpreadMid_evolution(df, ISIN1, ISIN2, column='MidYTM'):\n",
    "    \"\"\"\n",
    "    Plots the evolution of the 'column' value of a bond over its deal dates.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing bond data.\n",
    "    - ISIN (str): The International Securities Identification Number of the bond.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Process the dataframe\n",
    "    df_ISIN1 = df[df['ISIN'] == ISIN1].copy()\n",
    "    df_ISIN1.sort_values(by='Deal_Date', inplace=True)\n",
    "\n",
    "    df_ISIN2 = df[df['ISIN'] == ISIN2].copy()\n",
    "    df_ISIN2.sort_values(by='Deal_Date', inplace=True)\n",
    "\n",
    "    # Plot the figure with violet color scheme\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.plot(df_ISIN1['Deal_Date'], df_ISIN1[column], color='purple', linestyle='-', marker='o', markersize=4, linewidth=2, label=f'Initial ISIN ({ISIN1})')\n",
    "    plt.plot(df_ISIN2['Deal_Date'], df_ISIN2[column], color='purple', alpha=0.3, linestyle='-',  marker='o', markersize=4, label=f'Recommended ISIN ({ISIN2})')\n",
    "    plt.xlabel('Deal Date', fontsize=10)\n",
    "    plt.ylabel(f'{column}', fontsize=10)\n",
    "    plt.title(f'{column} as a function of the Deal Date', fontsize=13)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this function\n",
    "columns_to_compare = ['MidYTM', 'MidPrice', 'SpreadvsBenchmarkMid', 'MidASWSpread',\n",
    "                      'MidZSpread', 'GSpreadMid']\n",
    "\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "\n",
    "for feature in columns_to_compare :\n",
    "    plot_double_GSpreadMid_evolution(df_preprocessed, initial_ISIN, recommended_ISIN, column=feature) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_traded_volume(df, ISIN1, ISIN2):\n",
    "    \"\"\"\n",
    "    Plots the monthly traded volume for two given ISINs side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing bond data.\n",
    "    - ISIN1 (str): The International Securities Identification Number of the first bond.\n",
    "    - ISIN2 (str): The International Securities Identification Number of the second bond.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the dataframe for ISIN1\n",
    "    df_ISIN1 = df[df['ISIN'] == ISIN1].copy()\n",
    "    df_ISIN1['Deal_Date'] = pd.to_datetime(df_ISIN1['Deal_Date'])\n",
    "    df_ISIN1['Date'] = df_ISIN1['Deal_Date'].dt.to_period('M')\n",
    "    df_ISIN1 = df_ISIN1.groupby('Date')['Total_Traded_Volume'].sum().reset_index()\n",
    "\n",
    "    # Process the dataframe for ISIN2\n",
    "    df_ISIN2 = df[df['ISIN'] == ISIN2].copy()\n",
    "    df_ISIN2['Deal_Date'] = pd.to_datetime(df_ISIN2['Deal_Date'])\n",
    "    df_ISIN2['Date'] = df_ISIN2['Deal_Date'].dt.to_period('M')\n",
    "    df_ISIN2 = df_ISIN2.groupby('Date')['Total_Traded_Volume'].sum().reset_index()\n",
    "\n",
    "    # Plot side by side\n",
    "    plt.figure(figsize=(16, 5))\n",
    "\n",
    "    # Plot for ISIN1\n",
    "    plt.subplot(1, 2, 1)\n",
    "    ax1 = sns.barplot(x='Date', y='Total_Traded_Volume', data=df_ISIN1, color='purple')\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.title(f'Monthly Traded Volume for Initial ISIN: {ISIN1}\\n', fontsize=16)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Volume Traded')\n",
    "\n",
    "    # Plot for ISIN2\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ax2 = sns.barplot(x='Date', y='Total_Traded_Volume', data=df_ISIN2, color='purple')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.title(f'Monthly Traded Volume for Recommended ISIN: {ISIN2}\\n', fontsize=16)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Volume Traded')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "\n",
    "# Example usage:\n",
    "plot_monthly_traded_volume(df_preprocessed, initial_ISIN, recommended_ISIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed[df_preprocessed['ISIN']==initial_ISIN].sort_values('Deal_Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_traded_volume(df, ISIN1, ISIN2):\n",
    "    \"\"\"\n",
    "    Plots the monthly traded volume for two given ISINs side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing bond data.\n",
    "    - ISIN1 (str): The International Securities Identification Number of the first bond.\n",
    "    - ISIN2 (str): The International Securities Identification Number of the second bond.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Process the dataframe for ISIN1\n",
    "    df_ISIN1 = df[df['ISIN'] == ISIN1].copy()\n",
    "    df_ISIN1['Deal_Date'] = pd.to_datetime(df_ISIN1['Deal_Date'])\n",
    "    df_ISIN1['Date'] = df_ISIN1['Deal_Date'].dt.to_period('M')\n",
    "    df_ISIN1['transaction'] = 1\n",
    "    df_ISIN1 = df_ISIN1.groupby('Date')['transaction'].sum().reset_index()\n",
    "\n",
    "    # Process the dataframe for ISIN2\n",
    "    df_ISIN2 = df[df['ISIN'] == ISIN2].copy()\n",
    "    df_ISIN2['Deal_Date'] = pd.to_datetime(df_ISIN2['Deal_Date'])\n",
    "    df_ISIN2['Date'] = df_ISIN2['Deal_Date'].dt.to_period('M')\n",
    "    df_ISIN2['transaction'] = 1\n",
    "    df_ISIN2 = df_ISIN2.groupby('Date')['transaction'].sum().reset_index()\n",
    "\n",
    "    # Plot side by side\n",
    "    plt.figure(figsize=(16, 5))\n",
    "\n",
    "    # Plot for ISIN1\n",
    "    plt.subplot(1, 2, 1)\n",
    "    ax1 = sns.barplot(x='Date', y='transaction', data=df_ISIN1, color='purple')\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.title(f'Monthly RFQs for Initial ISIN: {ISIN1}\\n', fontsize=15)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total RFQs')\n",
    "\n",
    "    # Plot for ISIN2\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ax2 = sns.barplot(x='Date', y='transaction', data=df_ISIN2, color='purple')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.title(f'Monthly RFQs Volume for Recommended ISIN: {ISIN2}\\n', fontsize=15)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total RFQs')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "\n",
    "# Example usage:\n",
    "plot_monthly_traded_volume(df_preprocessed, initial_ISIN, recommended_ISIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_traded_volume(df, ISIN1, ISIN2):\n",
    "    \"\"\"\n",
    "    Plots the monthly traded volume for two given ISINs on the same plot using density curves.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing bond data.\n",
    "    - ISIN1 (str): The International Securities Identification Number of the first bond.\n",
    "    - ISIN2 (str): The International Securities Identification Number of the second bond.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Process the dataframe for ISIN1\n",
    "    df_ISIN1 = df[df['ISIN'] == ISIN1].copy()\n",
    "    df_ISIN1['Deal_Date'] = pd.to_datetime(df_ISIN1['Deal_Date'])\n",
    "    df_ISIN1['Date'] = df_ISIN1['Deal_Date'].dt.to_period('M')\n",
    "    df_ISIN1['transaction'] = 1\n",
    "    df_ISIN1 = df_ISIN1.groupby('Date')['transaction'].sum().reset_index()\n",
    "\n",
    "    # Process the dataframe for ISIN2\n",
    "    df_ISIN2 = df[df['ISIN'] == ISIN2].copy()\n",
    "    df_ISIN2['Deal_Date'] = pd.to_datetime(df_ISIN2['Deal_Date'])\n",
    "    df_ISIN2['Date'] = df_ISIN2['Deal_Date'].dt.to_period('M')\n",
    "    df_ISIN2['transaction'] = 1\n",
    "    df_ISIN2 = df_ISIN2.groupby('Date')['transaction'].sum().reset_index()\n",
    "\n",
    "    # Plot density curves on the same plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_ISIN1['Date'].astype('datetime64[ns]'), df_ISIN1['transaction'], label=f'Initial ISIN ({ISIN1})', color='purple', marker='o')\n",
    "    plt.plot(df_ISIN2['Date'].astype('datetime64[ns]'), df_ISIN2['transaction'], label=f'Recommended ISIN ({ISIN2})', color='green', marker='o')\n",
    "\n",
    "    plt.title(f'Monthly Number of Transactions\\n', fontsize=16)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of RFQs')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "initial_ISIN = 'XS1985806600'\n",
    "recommended_ISIN = 'XS1991125896'\n",
    "\n",
    "plot_monthly_traded_volume(df_preprocessed, initial_ISIN, recommended_ISIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ISIN = 'XS2236363573'\n",
    "recommended_ISIN = 'XS2194370727'\n",
    "\n",
    "df_initial_ISIN = df_merge[df_merge['ISIN'] == recommended_ISIN].copy()\n",
    "df_initial_ISIN['Deal_Date'] = pd.to_datetime(df_initial_ISIN['Deal_Date'])\n",
    "df_initial_ISIN_sell = df_initial_ISIN[df_initial_ISIN['B_Side']==0].copy()\n",
    "df_initial_ISIN_buy = df_initial_ISIN[df_initial_ISIN['B_Side']==1].copy()\n",
    "\n",
    "df_initial_ISIN_sell['Date'] = df_initial_ISIN_sell['Deal_Date'].dt.to_period('M')\n",
    "df_initial_ISIN_sell = df_initial_ISIN_sell.groupby('Date')['Total_Requested_Volume'].sum().reset_index()\n",
    "df_initial_ISIN_sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot demand evolution and traded volume on the same plot \n",
    "\n",
    "initial_ISIN = 'XS2236363573'\n",
    "\n",
    "# Process the dataframe for ISIN1\n",
    "df_initial_ISIN = df[df['ISIN'] == initial_ISIN].copy()\n",
    "columns_to_extract = ['Total_Requested_Volume', 'Total_Traded_Volume']\n",
    "df_initial_ISIN['Deal_Date'] = pd.to_datetime(df_initial_ISIN['Deal_Date'])\n",
    "df_initial_ISIN['Date'] = df_initial_ISIN['Deal_Date'].dt.to_period('M')\n",
    "df_initial_ISIN = df_initial_ISIN.groupby('Date')['Total_Traded_Volume'].sum().reset_index()\n",
    "df_initial_ISIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company side "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sectors he usually buys the bonds in \n",
    "- The average rating of the bonds he buys according to each rating \n",
    "- Transaction volume trends of the client, and of the client for this particular bond\n",
    "- Evolution of the average price of the bonds he buys\n",
    "- How many of all transactions he is responsible for \n",
    "- Is he more often a seller or a buyer?\n",
    "\n",
    "Tableau récapitulatif : \n",
    "- fréquence moyenne \n",
    "\n",
    "Does he often buy the same bond several times (recidivism)? Quelle fréquence ? Quelle type de bonds ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_ex1 = 'ose'\n",
    "client_ex2 = 'cooperative financiere'\n",
    "client_ex3 = 'societe generale mer'\n",
    "client_ex4 = 'bred'\n",
    "client_ex5 = 'scp laureau-jeannerot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def plot_client_industry_info(df, client_name):\n",
    "\n",
    "    df_client = df[df['company_short_name'] == client_name].copy()\n",
    "    df_client_unique_ISIN = df_client.groupby(['ISIN', 'Year_dealdate']).first().reset_index()\n",
    "    df_client_unique_ISIN.BloomIndustrySector.value_counts()\n",
    "    df_client_sector_table = df_client_unique_ISIN[['BloomIndustrySector', 'BloomIndustryGroup', 'BloomIndustrySubGroup', 'Year_dealdate']].copy()\n",
    "    df_client_sector_table_grouped = df_client_sector_table.groupby(['Year_dealdate', 'BloomIndustrySector']).size().reset_index(name='Count')\n",
    "\n",
    "    df_client_sector_order = df_client_sector_table_grouped.sort_values(by='Count', ascending=False)\n",
    "    industry_sectors = df_client_sector_order.BloomIndustrySector.unique()\n",
    "\n",
    "    years = df_client_sector_table_grouped.Year_dealdate.unique()\n",
    "    data = []\n",
    "    for year in years :\n",
    "        data_per_year = [year]\n",
    "        df_year = df_client_sector_table_grouped[df_client_sector_table_grouped['Year_dealdate'] == year].copy()\n",
    "        for industry in industry_sectors : \n",
    "            if not df_year[df_year['BloomIndustrySector']==industry].empty :\n",
    "                data_per_year.append(df_year[df_year['BloomIndustrySector']==industry].Count.iloc[0])\n",
    "            else :\n",
    "                data_per_year.append(0)\n",
    "        data.append(data_per_year)\n",
    "\n",
    "    # Create a fancy grid table using tabulate\n",
    "    table = tabulate(data, headers=industry_sectors,\n",
    "                     tablefmt='fancy_grid', numalign=\"center\", stralign=\"center\", colalign=(\"center\",),\n",
    "                     showindex=False)\n",
    "    print(f\"\\nMost represented industry's sectors for Client: {client_name}\\n\")\n",
    "\n",
    "    # Print the tabale\n",
    "    print(table)\n",
    "\n",
    "# Example usage:\n",
    "plot_client_industry_info(df_preprocessed, client_ex1)\n",
    "plot_client_industry_info(df_preprocessed, client_ex3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def plot_client_subindustry_info(df, client_name):\n",
    "\n",
    "    df_client = df[df['company_short_name'] == client_name].copy()\n",
    "    df_client_unique_ISIN = df_client.groupby(['ISIN', 'Year_dealdate']).first().reset_index()\n",
    "    df_client_unique_ISIN.BloomIndustrySector.value_counts()\n",
    "    df_client_sector_table = df_client_unique_ISIN[['BloomIndustrySector', 'BloomIndustryGroup', 'BloomIndustrySubGroup', 'Year_dealdate']].copy()\n",
    "    df_client_sector_table_grouped = df_client_sector_table.groupby(['Year_dealdate', 'BloomIndustryGroup']).size().reset_index(name='Count')\n",
    "\n",
    "    df_client_sector_order = df_client_sector_table_grouped.sort_values(by='Count', ascending=False)\n",
    "    industry_sectors = df_client_sector_order.BloomIndustryGroup.unique()\n",
    "\n",
    "    years = df_client_sector_table_grouped.Year_dealdate.unique()\n",
    "    data = []\n",
    "    for year in years :\n",
    "        data_per_year = [year]\n",
    "        df_year = df_client_sector_table_grouped[df_client_sector_table_grouped['Year_dealdate'] == year].copy()\n",
    "        for industry in industry_sectors : \n",
    "            if not df_year[df_year['BloomIndustryGroup']==industry].empty :\n",
    "                data_per_year.append(df_year[df_year['BloomIndustryGroup']==industry].Count.iloc[0])\n",
    "            else :\n",
    "                data_per_year.append(0)\n",
    "        data.append(data_per_year)\n",
    "\n",
    "    # Create a fancy grid table using tabulate\n",
    "    table = tabulate(data, headers=industry_sectors,\n",
    "                     tablefmt='fancy_grid', numalign=\"center\", stralign=\"center\", colalign=(\"center\",),\n",
    "                     showindex=False)\n",
    "    print(f\"\\nMost represented industry's sub group for Client: {client_name}\\n\")\n",
    "\n",
    "    # Print the tabale\n",
    "    print(table)\n",
    "\n",
    "# Example usage:\n",
    "plot_client_subindustry_info(df_preprocessed, client_ex1)\n",
    "plot_client_subindustry_info(df_preprocessed, client_ex2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
