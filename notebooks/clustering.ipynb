{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this notebook, we aim at creating a representation of all financial assets in the database that allows to group them according to some shared characteristics. This also includes dealing with categorical and null values as well as defining a preprocessing function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1째 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../RFQ_Data_Challenge_HEC.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2째 Defining the preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights on some features :\n",
    "- **Deal_Date** : The date on which a financial deal is executed. Needs to be converteted in datetime.\n",
    "- **ISIN** : International Securities Identification Number, a unique identifier for Financial instruments.\n",
    "- **company_short_name** : the name of the issuer of the financial instrument. It corresponds to the client name.\n",
    "- **B_price** : The bid price of the financial instrument. To be converted to int (object for the moment)\n",
    "- **B_side** : Natixis's position as a buyer or seller of the financial instrument (for the moment 'NATIXIS SELL' or 'NATIXIS BUY'). Contient 8 valeurs nulles.\n",
    "- **Total_Requested_Volume** : The requested volume for buying or selling the financial instrument. It needs to be converted into a numerical column (object for the moment). Contient 2 valeurs nulles.\n",
    "- **Total_Traded_Volume_Natixis** : The volume of the financial instrument traded by Natixis. Already good data type.\n",
    "- **Total_Traded_Volume_Away** : The volume of the financial instrument traded by other banks. Already good data type.\n",
    "- **Total_Traded_Volume** : The total volume of the financial instrument traded. Already good data type.\n",
    "- **BloomIndustrySector**, **BloomIndustryGroup**,**BloomIndustrySubGroup**\n",
    "- **maturity** : The length of time during which interest is paid. Some null values are marked as NaT. We convert this column into Datetime type. Some maturities go back to 1900, it is not possible. We delete those.\n",
    "- **Rating_Fitch** : The credit rating of the financial instrument from Fitch Ratings.\n",
    "- **Rating_Moodys** : The credit rating of the financial instrument from Moody's.\n",
    "- **Rating_SP** : The credit rating of the financial instrument from S&P Global Ratings.\n",
    "- **Ccy** : The currency in which the financial instrument is denominated.\n",
    "- **Classification** : The activity sector of the company.\n",
    "- **Tier** : The seniority level of the financial instrument. Lots of null value, we replace them by UNKOWN (627100 values)\n",
    "- **AssumedMaturity** : The assumed maturity date of the financial instrument. Also contains a lot of null values, we replace them by maturity values (the null values only).\n",
    "- **Coupon** : The interest rate of the financial instrument. Already a float.\n",
    "- **Frequency** : The frequency of interest payments on the financial instrument. Takes values 1M, 3M, 6M, 12M. We delete the 'M' and convert the value into int.\n",
    "- **Type** : The type of interest rate on the financial instrument (fixed or variable).\n",
    "- **MidYTM** : The yield to maturity on the prime bid. Already a float.\n",
    "- **MidYTM** : The yield to maturity on the prime bid. Already a float.\n",
    "- **YTWDate** : Yield to Worst - The yield on the first possible redemption date. \n",
    "- **SpreadvsBenchmarkMid** : The spread of the financial instrument versus the interpolated government bond curve.\n",
    "- **MidASWSpread** : The spread of the financial instrument versus the swap curve.\n",
    "- **MidZSpread** : The spread of the financial instrument versus the zero- coupon curve.\n",
    "- **GSpreadMid** : The spread of the financial instrument versus the interpolated government bond curve.\n",
    "- **MidModifiedDuration** : The modified duration of the financial instrument. \n",
    "- **MidConvexity** : The convexity of the financial instrument.\n",
    "- **MidEffectiveDuration** : The effective duration of the financial instrument.\n",
    "- **MidEffectiveConvexity** : The effective convexity of the financial instrument.\n",
    "\n",
    "Features that can be deleted at first : \n",
    "- **Cusip**, same as **cusip** but with more null values \n",
    "- **Maturity**, same as **maturity**\n",
    "\n",
    "Added columns :\n",
    "- Year, month, day of deal_date\n",
    "- Year, month, day of maturity\n",
    "- days to maturity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values analysis for columns with null values below 15000\n",
    "below_threshold = df.isnull().sum().sort_values(ascending=False) < 15000\n",
    "print(\"Columns with null values below 15000:\")\n",
    "print(below_threshold[below_threshold].index)\n",
    "\n",
    "# Null values analysis for columns with null values above 15000\n",
    "above_threshold = df.isnull().sum().sort_values(ascending=False) >= 15000\n",
    "print(\"\\nColumns with null values above or equal to 15000:\")\n",
    "print(above_threshold[above_threshold].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame with the following steps:\n",
    "    1. Converts 'Deal_Date', 'maturity', 'AssumedMaturity', 'YTWDate' columns to datetime.\n",
    "    2. Converts 'B_Side' column to boolean (1 for 'NATIXIS BUY', 0 for 'NATIXIS SELL').\n",
    "    3. Converts 'B_Price' and 'Total_Requested_Volume' columns to integers.\n",
    "    4. Fills null values in 'Tier', 'AssumedMaturity', and 'YTWDate' columns with 'UNKNOWN'.\n",
    "    5. Converts 'Frequency' feature values into integers (removing 'M' from the end).\n",
    "    6. Drops the unsused 'Cusip' column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop null values only for columns below the threshold\n",
    "    columns_to_delete_null_vales = ['MidYTM', 'Coupon', 'Ccy', 'cusip',\n",
    "       'maturity', 'cdcissuerShortName', 'Frequency', 'MidPrice', 'cdcissuer',\n",
    "       'company_short_name', 'BloomIndustrySubGroup', 'B_Price',\n",
    "       'Total_Traded_Volume_Natixis', 'B_Side',\n",
    "       'Total_Traded_Volume_Away', 'Total_Requested_Volume',\n",
    "       'Total_Traded_Volume', 'Type', 'Maturity', 'ISIN', 'Deal_Date']\n",
    "    df = df.dropna(subset=columns_to_delete_null_vales)\n",
    "\n",
    "    # Convert 'B_Price', 'Total_Requested_Volume', 'Frequency' to integers\n",
    "    df['Frequency'] = df['Frequency'].str.replace('M', '')\n",
    "    numerical_columns = ['B_Price', 'Total_Requested_Volume', 'Frequency']\n",
    "    df.dropna(subset=numerical_columns, inplace=True)\n",
    "    for column in numerical_columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce').astype(int)\n",
    "\n",
    "    # Fix the error in the B_Price column\n",
    "    df = df[df['B_Price'] >= 20]\n",
    "\n",
    "    # Replace NaT with null values in the 'Maturity' column\n",
    "    df['maturity'].replace({pd.NaT: np.nan}, inplace=True)\n",
    "\n",
    "    # Convert 'Deal_Date', 'maturity', 'AssumedMaturity', 'YTWDate' to datetime\n",
    "    df['Deal_Date'] = pd.to_datetime(df['Deal_Date'])\n",
    "    df['maturity'] = pd.to_datetime(df['maturity'], errors='coerce',  format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df['AssumedMaturity'] = pd.to_datetime(df['AssumedMaturity'], errors='coerce')\n",
    "    df['YTWDate'] = pd.to_datetime(df['YTWDate'], errors='coerce')\n",
    "\n",
    "    # Add year, month, day for clustering \n",
    "    df['Year_dealdate'] = df['Deal_Date'].dt.year\n",
    "    df['Month_dealdate'] = df['Deal_Date'].dt.month\n",
    "    df['Day_dealdate'] = df['Deal_Date'].dt.day\n",
    "    df['Year_maturity'] = df['maturity'].dt.year\n",
    "    df['Month_maturity'] = df['maturity'].dt.month\n",
    "    df['Day_maturity'] = df['maturity'].dt.day\n",
    "\n",
    "    # Delete maturities smaller than 2021 (as deal dates starts in 2021)\n",
    "    df = df[df['maturity'].dt.year >= 2021]\n",
    "\n",
    "    # Compute number of days between maturity and deal date\n",
    "    df['Days_to_Maturity'] = (df['maturity'] - df['Deal_Date']).dt.days\n",
    "\n",
    "    # Replace null values in 'AssumedMaturity' with values from 'Maturity'\n",
    "    df['AssumedMaturity'] = df['AssumedMaturity'].fillna(df['Maturity'])\n",
    "\n",
    "    # Convert 'B_Side' column to boolean (1 for 'NATIXIS BUY', 0 for 'NATIXIS SELL')\n",
    "    df = df[df['B_Side'].isin(['NATIXIS SELL', 'NATIXIS BUY'])]\n",
    "    df['B_Side'] = df['B_Side'].replace({'NATIXIS BUY': 1, 'NATIXIS SELL': 0})\n",
    "\n",
    "    # Convert null values of 'Tier'\n",
    "    df['Tier'].fillna('UNKNOWN', inplace=True)\n",
    "\n",
    "    # Lower string names \n",
    "    df['Sales_Name'] = df['Sales_Name'].str.lower()\n",
    "    df['company_short_name'] = df['company_short_name'].str.lower()\n",
    "\n",
    "    # Drop unused columns\n",
    "    columns_to_drop = ['Cusip', 'Maturity']\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = preprocess_dataframe(df)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for imputing numerical missing values in the financial columns\n",
    "def complete_nan_values(df):\n",
    "\n",
    "    df_unique_isin = df.groupby('ISIN').first()\n",
    "    columns = ['Classification', 'SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "               'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity', 'Year_dealdate', 'Month_dealdate']\n",
    "    df_by_classification = df_unique_isin[columns].copy()\n",
    "    df_by_classification = df_by_classification.groupby(['Classification', 'Year_dealdate']).mean().reset_index()\n",
    "\n",
    "    df_group_by_industry = df_by_classification.groupby('Classification').mean().reset_index()\n",
    "    numeric_columns = ['SpreadvsBenchmarkMid', 'MidASWSpread', 'MidZSpread', 'GSpreadMid', \n",
    "                       'MidModifiedDuration', 'MidConvexity', 'MidEffectiveDuration', 'MidEffectiveConvexity']\n",
    "    \n",
    "    df_by_classification['additional_column'] = df_by_classification['Classification'].astype(str) + ' - ' + df_by_classification['Year_dealdate'].astype(str)\n",
    "    df['additional_column'] = df['Classification'].astype(str) + ' - ' + df['Year_dealdate'].astype(str)\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        df_by_classification[column] = df_by_classification[column].fillna(df_by_classification['Classification'].map(df_group_by_industry.set_index('Classification')[column]))\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        df[column] = df[column].fillna(df['additional_column'].map(df_by_classification.set_index('additional_column')[column]))\n",
    "\n",
    "    df.drop(columns=['additional_column'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = complete_nan_values(df_preprocessed)\n",
    "missing_values = df_filled.isnull().sum()\n",
    "missing_values[missing_values!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've corrected the B_price error, which sometimes corresponds to yield values (by setting a minimum value of 20), only 314718 lines remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4째 Defining the preprocessing function for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clustering(df, cols_to_exclude):\n",
    "\n",
    "    # Drop the columns that we exclude\n",
    "    df = df.drop(cols_to_exclude, axis=1, errors='ignore')\n",
    "\n",
    "    # Identify numerical columns\n",
    "    numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Transform 'Ccy' to 'is_euro' boolean column\n",
    "    df['is_euro'] = (df['Ccy'] == 'EUR').astype(int)\n",
    "    # Transform 'Type' to 'is_fixed' boolean column\n",
    "    df['is_fixed'] = (df['Type'] == 'Fixed').astype(int)\n",
    "    # Drop the original 'Ccy' and 'Type' columns\n",
    "    df = df.drop(['Ccy', 'Type'], axis=1, errors='ignore')\n",
    "\n",
    "    # Ordinal encoding for 'Rating_Fitch'\n",
    "    rating_mapping = {\n",
    "        'AAA': 22,\n",
    "        'AA+': 21,\n",
    "        'AA': 20,\n",
    "        'AA-': 19,\n",
    "        'A+': 18,\n",
    "        'A': 17,\n",
    "        'A-': 16,\n",
    "        'BBB+': 15,\n",
    "        'BBB': 14,\n",
    "        'BBB-': 13,\n",
    "        'BB+': 12,\n",
    "        'BB': 11,\n",
    "        'BB-': 10,\n",
    "        'B+': 9,\n",
    "        'B': 8,\n",
    "        'B-': 7,\n",
    "        'CCC+': 6,\n",
    "        'CCC': 5,\n",
    "        'CCC-': 4,\n",
    "        'CC': 3,\n",
    "        'C': 2,\n",
    "        'WD': 1,\n",
    "        'D': 0,\n",
    "        'NR': np.nan\n",
    "    }\n",
    "\n",
    "    rating_mapping_moodys = {\n",
    "        'Aaa': 22,\n",
    "        'Aa1': 21,\n",
    "        'Aa2': 20,\n",
    "        '(P)Aa2': 20,\n",
    "        'Aa3': 19,\n",
    "        '(P)Aa3': 19,\n",
    "        'A1': 18,\n",
    "        '(P)A1': 18,\n",
    "        'A2': 17,\n",
    "        '(P)A2': 17,\n",
    "        'A3': 16,\n",
    "        '(P)A3': 16,\n",
    "        'Baa1': 15,\n",
    "        '(P)Baa1': 15,\n",
    "        'Baa2': 14,\n",
    "        '(P)Baa2': 14,\n",
    "        'Baa3': 13,\n",
    "        'Ba1': 12,\n",
    "        'Ba2': 11,\n",
    "        'Ba3': 10,\n",
    "        'B1': 9,\n",
    "        'B2': 8,\n",
    "        'B3': 7,\n",
    "        'Caa1': 6,\n",
    "        'Caa2': 5,\n",
    "        'Caa3': 4,\n",
    "        'Ca': 2.5,\n",
    "        'C': 0\n",
    "    }\n",
    "\n",
    "    df['Rating_Fitch_encoded'] = df['Rating_Fitch'].map(rating_mapping)\n",
    "    df['Rating_SP_encoded'] = df['Rating_SP'].map(rating_mapping)\n",
    "    df['Rating_Moodys_encoded'] = df['Rating_Moodys'].map(rating_mapping_moodys)\n",
    "    # Create a unique Rating that averages the 3 Ratings and ignores missing values\n",
    "    df['Rating'] = df[['Rating_Fitch_encoded', 'Rating_SP_encoded', 'Rating_Moodys_encoded']].mean(axis=1)\n",
    "\n",
    "    # Map values in 'Country' column\n",
    "    valid_countries = ['FRANCE', 'ITALY', 'GERMANY', 'NETHERLANDS', 'SPAIN']\n",
    "    df['Country'] = df['Country'].apply(lambda x: x if x in valid_countries else 'OTHER')\n",
    "    # Perform one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=['Country'], prefix='is')\n",
    "\n",
    "    # Map values in 'Classification' column\n",
    "    valid_classes = ['Financials', 'Government', 'Industrials', 'Utilities']\n",
    "    df['Classification'] = df['Classification'].apply(lambda x: x if x in valid_classes else 'OTHER')\n",
    "    # Perform one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=['Classification'], prefix='is')\n",
    "\n",
    "    # Add newly created boolean columns and 'Rating' to agg_dict with average\n",
    "    agg_dict = {col: 'mean' for col in ['is_euro', 'is_fixed', 'Rating']}\n",
    "    agg_dict.update({col: 'first' for col in ['is_FRANCE', 'is_ITALY', 'is_GERMANY', 'is_NETHERLANDS', 'is_SPAIN']})\n",
    "    agg_dict.update({col: 'first' for col in ['is_Financials', 'is_Government', 'is_Industrials', 'is_Utilities']})\n",
    "    agg_dict.update({num_col: ['min', 'max', 'median'] for num_col in numerical_columns})\n",
    "\n",
    "    # Grouping by 'ISIN' and aggregating columns\n",
    "    grouped_df = df.groupby('ISIN').agg(agg_dict).reset_index()\n",
    "\n",
    "    # Flatten the multi-level column index\n",
    "    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "\n",
    "    # Drop identical columns\n",
    "    grouped_df = grouped_df.T.drop_duplicates().T\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = ['Deal_Date', 'cusip', 'B_Side', 'Instrument', 'Sales_Name', 'Sales_Initial', 'company_short_name',\n",
    "                   'Total_Requested_Volume', 'Total_Traded_Volume_Natixis', 'Total_Traded_Volume_Away', 'Total_Traded_Volume',\n",
    "                   'cdissuer', 'Tier', 'Year_dealdate', 'Month_dealdate','Day_dealdate', 'Days_to_Maturity',\n",
    "                   'cdissuerShortName', 'lb_Platform_2']\n",
    "df_clustering = preprocess_clustering(df_filled, cols_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5째 Supervised clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering.set_index('ISIN_', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now deep dive into the classical KMeans where we will be imputing the missing values. Financial missing values have been imputed previously. We just need to impute the ratings. We will proceed with the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_clustering.isnull().sum()\n",
    "missing_values[missing_values!=0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_filled = df_clustering.copy()\n",
    "df_clustering_filled['Rating_mean'] = df_clustering_filled['Rating_mean'].fillna(df_clustering['Rating_mean'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_normalized = scaler.fit_transform(df_clustering_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the elbow method to determine the optimal number of clusters for the KMeans approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from umap.umap_ import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(4,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.fit(df_normalized)        # Fit the data to the visualizer\n",
    "visualizer.show();                          # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal number of cluster is 7.\n",
    "<br>\n",
    "Now let's move on to exploring the results we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=7, verbose=0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = model.fit_predict(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = UMAP(n_neighbors=50, learning_rate=0.5, init=\"random\", min_dist=0.001\n",
    "                      ).fit_transform(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=embedding[:,0], y=embedding[:,1], hue=clusters, palette='dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainability options:\n",
    "- ExKMC\n",
    "- Build a classification model for each label and look at Shap values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an explainability classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_explainability(df, cols_to_exclude):\n",
    "\n",
    "    # Drop the columns that we exclude\n",
    "    df = df.drop(cols_to_exclude, axis=1, errors='ignore')\n",
    "\n",
    "    # Identify numerical columns\n",
    "    numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Transform 'Ccy' to 'is_euro' boolean column\n",
    "    df['is_euro'] = (df['Ccy'] == 'EUR').astype(int)\n",
    "    # Transform 'Type' to 'is_fixed' boolean column\n",
    "    df['is_fixed'] = (df['Type'] == 'Fixed').astype(int)\n",
    "    # Drop the original 'Ccy' and 'Type' columns\n",
    "    df = df.drop(['Ccy', 'Type'], axis=1, errors='ignore')\n",
    "\n",
    "    # Ordinal encoding for 'Rating_Fitch'\n",
    "    rating_mapping = {\n",
    "        'AAA': 22,\n",
    "        'AA+': 21,\n",
    "        'AA': 20,\n",
    "        'AA-': 19,\n",
    "        'A+': 18,\n",
    "        'A': 17,\n",
    "        'A-': 16,\n",
    "        'BBB+': 15,\n",
    "        'BBB': 14,\n",
    "        'BBB-': 13,\n",
    "        'BB+': 12,\n",
    "        'BB': 11,\n",
    "        'BB-': 10,\n",
    "        'B+': 9,\n",
    "        'B': 8,\n",
    "        'B-': 7,\n",
    "        'CCC+': 6,\n",
    "        'CCC': 5,\n",
    "        'CCC-': 4,\n",
    "        'CC': 3,\n",
    "        'C': 2,\n",
    "        'WD': 1,\n",
    "        'D': 0,\n",
    "        'NR': np.nan\n",
    "    }\n",
    "\n",
    "    rating_mapping_moodys = {\n",
    "        'Aaa': 22,\n",
    "        'Aa1': 21,\n",
    "        'Aa2': 20,\n",
    "        '(P)Aa2': 20,\n",
    "        'Aa3': 19,\n",
    "        '(P)Aa3': 19,\n",
    "        'A1': 18,\n",
    "        '(P)A1': 18,\n",
    "        'A2': 17,\n",
    "        '(P)A2': 17,\n",
    "        'A3': 16,\n",
    "        '(P)A3': 16,\n",
    "        'Baa1': 15,\n",
    "        '(P)Baa1': 15,\n",
    "        'Baa2': 14,\n",
    "        '(P)Baa2': 14,\n",
    "        'Baa3': 13,\n",
    "        'Ba1': 12,\n",
    "        'Ba2': 11,\n",
    "        'Ba3': 10,\n",
    "        'B1': 9,\n",
    "        'B2': 8,\n",
    "        'B3': 7,\n",
    "        'Caa1': 6,\n",
    "        'Caa2': 5,\n",
    "        'Caa3': 4,\n",
    "        'Ca': 2.5,\n",
    "        'C': 0\n",
    "    }\n",
    "\n",
    "    df['Rating_Fitch_encoded'] = df['Rating_Fitch'].map(rating_mapping)\n",
    "    df['Rating_SP_encoded'] = df['Rating_SP'].map(rating_mapping)\n",
    "    df['Rating_Moodys_encoded'] = df['Rating_Moodys'].map(rating_mapping_moodys)\n",
    "    # Create a unique Rating that averages the 3 Ratings and ignores missing values\n",
    "    df['Rating'] = df[['Rating_Fitch_encoded', 'Rating_SP_encoded', 'Rating_Moodys_encoded']].mean(axis=1)\n",
    "\n",
    "    # Add newly created boolean columns and 'Rating' to agg_dict with average\n",
    "    agg_dict = {col: 'mean' for col in ['is_euro', 'is_fixed', 'Rating']}\n",
    "    agg_dict.update({col: 'first' for col in ['Country', 'Classification']})\n",
    "    agg_dict.update({num_col: ['min', 'max', 'median'] for num_col in numerical_columns})\n",
    "\n",
    "    # Grouping by 'ISIN' and aggregating columns\n",
    "    grouped_df = df.groupby('ISIN').agg(agg_dict).reset_index()\n",
    "\n",
    "    # Flatten the multi-level column index\n",
    "    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "\n",
    "    # Drop identical columns\n",
    "    grouped_df = grouped_df.T.drop_duplicates().T\n",
    "\n",
    "    # Set back data types to numerical when needed\n",
    "    grouped_df = grouped_df.astype({col: 'float' for col in grouped_df.columns if col not in ['Classification_first', 'Country_first', 'ISIN_']})\n",
    "\n",
    "    # Replace missing values with empty string\n",
    "    grouped_df['Country_first'].replace({None: ''}, inplace=True)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = preprocess_explainability(df_filled, cols_to_exclude)\n",
    "df_exp['cluster'] = clusters\n",
    "df_exp.set_index(['ISIN_'], inplace=True)\n",
    "df_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to train CatBoostClassifier for each cluster label\n",
    "def train_catboost_classifier(df, cluster_labels, min_representation=100):\n",
    "    classifiers = {}\n",
    "\n",
    "    for label in cluster_labels:\n",
    "        # Check if the cluster label is represented at least min_representation times\n",
    "        if df['cluster'].value_counts().get(label, 0) < min_representation:\n",
    "            print(f\"Skipping cluster {label} as it has less than {min_representation} instances.\")\n",
    "            continue\n",
    "\n",
    "        # Create binary labels for the current cluster\n",
    "        lb = LabelBinarizer()\n",
    "        binary_labels = lb.fit_transform(df['cluster'] == label).ravel()\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df.drop('cluster', axis=1), binary_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Initialize CatBoostClassifier\n",
    "        clf = CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, loss_function='Logloss')\n",
    "\n",
    "        # Train the classifier\n",
    "        clf.fit(X_train, y_train, cat_features=['Country_first', 'Classification_first'], verbose=False)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        # Store the classifier and evaluation results\n",
    "        classifiers[label] = {'classifier': clf, 'accuracy': accuracy, 'classification_report': report}\n",
    "\n",
    "        # Retrain the model on the full data\n",
    "        clf.fit(df.drop('cluster', axis=1), binary_labels, cat_features=['Country_first', 'Classification_first'], verbose=False)\n",
    "\n",
    "        # Save the retrained model in classifiers dict\n",
    "        classifiers[label]['classifier_full_data'] = clf\n",
    "\n",
    "    return classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoostClassifier for each cluster label\n",
    "cluster_classifiers = train_catboost_classifier(df_exp, cluster_labels=df_exp['cluster'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explainability(model, df):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(df)\n",
    "\n",
    "    # Display beeswarm SHAP plot\n",
    "    #shap.plots.beeswarm(shap_values)\n",
    "    shap.summary_plot(shap_values, df, plot_type=\"beeswarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explainability_corrected(model, df):\n",
    "    \n",
    "    df_to_plot = df.reset_index().copy()\n",
    "    ISIN_list = df_to_plot.ISIN_.to_list()\n",
    "    index = pd.Index(ISIN_list)\n",
    "    df_to_plot.drop(columns=[\"ISIN_\"], inplace=True)\n",
    "    df_to_plot = df_to_plot.set_index(index)\n",
    "    df_to_plot = df_to_plot.drop('cluster', axis=1)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer(df_to_plot)\n",
    "    shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming cluster_classifiers is already defined\n",
    "for label, info in cluster_classifiers.items():\n",
    "    print(f\"Classifier for Cluster {label}:\")\n",
    "    print(f\"Accuracy: {info['accuracy']:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(info['classification_report'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Check if 'classifier_full_data' key exists in the dictionary\n",
    "    if 'classifier_full_data' in info:\n",
    "        print(f\"Plotting SHAP explainability for Cluster {label}\")\n",
    "        # Access the retrained model on full data\n",
    "        full_data_model = info['classifier_full_data']\n",
    "        \n",
    "        # Plot SHAP explainability\n",
    "        plot_shap_explainability_corrected(full_data_model, df_exp)\n",
    "        \n",
    "        plt.show()  # Display the plot\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"No 'classifier_full_data' available for Cluster {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommending similar bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_test = df_clustering_filled.reset_index().copy()\n",
    "ISIN_list = df_to_test.ISIN_.to_list()\n",
    "index = pd.Index(ISIN_list)\n",
    "df_to_test.drop(columns=[\"ISIN_\"], inplace=True)\n",
    "df_to_test = df_to_test.set_index(index)\n",
    "df_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def recommend_n_bonds(row_id, df, kmeans_model):\n",
    "    # Get the cluster of the given row\n",
    "    cluster_id = kmeans_model.predict([df.loc[row_id]])[0]\n",
    "    # Get the indices of data points in the same cluster\n",
    "    cluster_indices = np.where(kmeans_model.labels_ == cluster_id)[0]\n",
    "    # Get the distances between the given row and all other points in the cluster\n",
    "    distances = pairwise_distances(df.loc[[row_id]], df.iloc[cluster_indices], metric='euclidean')[0]\n",
    "    # Sort indices based on distances and get the top 5 nearest indices\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    top5_nearest_indices = cluster_indices[sorted_indices][:5]\n",
    "\n",
    "    return top5_nearest_indices.tolist()\n",
    "\n",
    "# Example usage:\n",
    "#row_id_to_check = 0  # Replace with the desired row index\n",
    "#top5_nearest_ids = recommend_n_bonds('AT0000383864', df_to_test, model)\n",
    "\n",
    "#print(f\"Top 5 nearest ids for row {row_id_to_check}: {top5_nearest_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(df_clustering_filled.loc['XS2717309855'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_nearest_ids = recommend_n_bonds('XS2717309855', df_to_test, model)\n",
    "\n",
    "print(f\"Top 5 nearest ids for row {row_id_to_check}: {top5_nearest_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['ISIN'] == 'XS2236363573')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.copy()\n",
    "df_test['B_Price'] = float(df['B_Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed[df_preprocessed['ISIN']=='XS2236363573']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_normalized, columns=df_clustering_filled.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=embedding[:,0], y=embedding[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=5, gen_min_span_tree=True)\n",
    "clusters = clusterer.fit_predict(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=embedding[:,0], y=embedding[:,1], hue=clusters, palette='dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for Natixis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond = df[df['ISIN']=='XS2236363573']\n",
    "df_bond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bond(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame with the following steps:\n",
    "    1. Converts 'Deal_Date', 'maturity', 'AssumedMaturity', 'YTWDate' columns to datetime.\n",
    "    2. Converts 'B_Side' column to boolean (1 for 'NATIXIS BUY', 0 for 'NATIXIS SELL').\n",
    "    3. Converts 'B_Price' and 'Total_Requested_Volume' columns to integers.\n",
    "    4. Fills null values in 'Tier', 'AssumedMaturity', and 'YTWDate' columns with 'UNKNOWN'.\n",
    "    5. Converts 'Frequency' feature values into integers (removing 'M' from the end).\n",
    "    6. Drops the unsused 'Cusip' column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Shift back the columns to the correct place\n",
    "    column_names = df.columns\n",
    "    # Find the index of 'cdIssuerShortName' and 'maturity'\n",
    "    cd_issuer_index = column_names.get_loc('cdcissuerShortName')\n",
    "    maturity_index = column_names.get_loc('maturity')\n",
    "    # Loop through each column and shift the data to the left\n",
    "    for i in range(cd_issuer_index, maturity_index + 1):\n",
    "        df.iloc[:, i] = df.iloc[:, i + 1]\n",
    "\n",
    "    # Replace empty column with nans\n",
    "    df.iloc[:, maturity_index+1] = np.nan\n",
    "\n",
    "    # Convert 'B_Price', 'Total_Requested_Volume', 'Frequency' to integers\n",
    "    df['Frequency'] = df['Frequency'].str.replace('M', '')\n",
    "    numerical_columns = ['B_Price', 'Total_Requested_Volume', 'Frequency']\n",
    "    #df.dropna(subset=numerical_columns, inplace=True)\n",
    "    for column in numerical_columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce').astype(int)\n",
    "\n",
    "    # Fix the error in the B_Price column\n",
    "    #df = df[df['B_Price'] >= 20]\n",
    "\n",
    "    # Replace NaT with null values in the 'Maturity' column\n",
    "    df['maturity'].replace({pd.NaT: np.nan}, inplace=True)\n",
    "\n",
    "    # Convert 'Deal_Date', 'maturity', 'AssumedMaturity', 'YTWDate' to datetime\n",
    "    df['Deal_Date'] = pd.to_datetime(df['Deal_Date'])\n",
    "    df['maturity'] = pd.to_datetime(df['maturity'], errors='coerce',  format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df['AssumedMaturity'] = pd.to_datetime(df['AssumedMaturity'], errors='coerce')\n",
    "    df['YTWDate'] = pd.to_datetime(df['YTWDate'], errors='coerce')\n",
    "\n",
    "    # Add year, month, day for clustering \n",
    "    df['Year_dealdate'] = df['Deal_Date'].dt.year\n",
    "    df['Month_dealdate'] = df['Deal_Date'].dt.month\n",
    "    df['Day_dealdate'] = df['Deal_Date'].dt.day\n",
    "    df['Year_maturity'] = df['maturity'].dt.year\n",
    "    df['Month_maturity'] = df['maturity'].dt.month\n",
    "    df['Day_maturity'] = df['maturity'].dt.day\n",
    "\n",
    "    # Delete maturities smaller than 2021 (as deal dates starts in 2021)\n",
    "    #df = df[df['maturity'].dt.year >= 2021]\n",
    "\n",
    "    # Compute number of days between maturity and deal date\n",
    "    df['Days_to_Maturity'] = (df['maturity'] - df['Deal_Date']).dt.days\n",
    "\n",
    "    # Replace null values in 'AssumedMaturity' with values from 'Maturity'\n",
    "    df['AssumedMaturity'] = df['AssumedMaturity'].fillna(df['Maturity'])\n",
    "\n",
    "    # Convert 'B_Side' column to boolean (1 for 'NATIXIS BUY', 0 for 'NATIXIS SELL')\n",
    "    df = df[df['B_Side'].isin(['NATIXIS SELL', 'NATIXIS BUY'])]\n",
    "    df['B_Side'] = df['B_Side'].replace({'NATIXIS BUY': 1, 'NATIXIS SELL': 0})\n",
    "\n",
    "    # Convert null values of 'Tier'\n",
    "    df['Tier'].fillna('UNKNOWN', inplace=True)\n",
    "\n",
    "    # Lower string names \n",
    "    df['Sales_Name'] = df['Sales_Name'].str.lower()\n",
    "    df['company_short_name'] = df['company_short_name'].str.lower()\n",
    "\n",
    "    # Drop unused columns\n",
    "    columns_to_drop = ['Cusip', 'Maturity']\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond = preprocess_bond(df_bond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond = pd.concat([df_preprocessed, df_bond], axis=0)\n",
    "df_bond.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond_filled = complete_nan_values(df_bond)\n",
    "df_bond.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute the missing values for price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'B_Price' and 'MidPrice' are columns in your DataFrame 'df'\n",
    "correlation = df_bond['B_Price'].corr(df_bond['MidPrice'])\n",
    "\n",
    "print(f\"Correlation between B_Price and MidPrice: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'B_Price' is the independent variable and 'MidPrice' is the dependent variable\n",
    "X = df_bond[['MidPrice']]\n",
    "y = df_bond['B_Price']\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print the coefficients\n",
    "p_intercept = model.intercept_\n",
    "p_slope = model.coef_[0]\n",
    "\n",
    "print(f\"Intercept: {p_intercept:.4f}\")\n",
    "print(f\"Slope (Coefficient for B_Price): {p_slope:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set the missing prices as price = 0.9896 * MidPrice + 0.5937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lambda function to calculate the predicted values\n",
    "fill_zero = lambda mid_price: p_intercept + p_slope * mid_price\n",
    "\n",
    "# Create a boolean mask for values equal to 0 in 'B_Price'\n",
    "mask = df_bond['B_Price'] == 0\n",
    "\n",
    "# Apply the lambda function to replace zero values in 'B_Price'\n",
    "df_bond.loc[mask, 'B_Price'] = df_bond.loc[mask, 'MidPrice'].apply(fill_zero)\n",
    "\n",
    "df_bond.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = ['Deal_Date', 'cusip', 'B_Side', 'Instrument', 'Sales_Name', 'Sales_Initial', 'company_short_name',\n",
    "                   'Total_Requested_Volume', 'Total_Traded_Volume_Natixis', 'Total_Traded_Volume_Away', 'Total_Traded_Volume',\n",
    "                   'cdissuer', 'Tier', 'Year_dealdate', 'Month_dealdate','Day_dealdate', 'Days_to_Maturity',\n",
    "                   'cdissuerShortName', 'lb_Platform_2']\n",
    "df_bond_clustering = preprocess_clustering(df_bond, cols_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond_clustering[df_bond_clustering['ISIN_']=='XS2236363573']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_bond_clustering.isnull().sum()\n",
    "missing_values[missing_values!=0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond_clustering_filled = df_bond_clustering.copy()\n",
    "df_bond_clustering_filled['Rating_mean'] = df_bond_clustering['Rating_mean'].fillna(df_bond_clustering['Rating_mean'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df_bond_clustering_filled.drop(columns=['ISIN_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_normalized = scaler.fit_transform(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_normalized, columns=df_bond_clustering_filled.columns[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=7)\n",
    "clusterer.fit(df_normalized)\n",
    "clusters = clusterer.predict(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond_clustering_filled['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond_clustering_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond_clustering_filled[df_bond_clustering_filled['ISIN_']=='XS2236363573']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def recommend_n_bonds(row_id, df, kmeans_model):\n",
    "    # Get the cluster of the given row\n",
    "    cluster_id = kmeans_model.predict([df.loc[row_id]])[0]\n",
    "    # Get the indices of data points in the same cluster\n",
    "    cluster_indices = np.where(kmeans_model.labels_ == cluster_id)[0]\n",
    "    # Get the distances between the given row and all other points in the cluster\n",
    "    distances = pairwise_distances(df.loc[[row_id]], df.iloc[cluster_indices], metric='euclidean')[0]\n",
    "    # Sort indices based on distances and get the top 5 nearest indices\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    top5_nearest_indices = cluster_indices[sorted_indices][:5]\n",
    "\n",
    "    return top5_nearest_indices.tolist()\n",
    "\n",
    "# Example usage:\n",
    "#row_id_to_check = 0  # Replace with the desired row index\n",
    "#top5_nearest_ids = recommend_n_bonds('AT0000383864', df_to_test, model)\n",
    "\n",
    "#print(f\"Top 5 nearest ids for row {row_id_to_check}: {top5_nearest_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def get_nearest_rows(df_normalized, isin_string):\n",
    "    # Find the index of the given ISIN string in df_bond_clustering_filled\n",
    "    index = df_bond_clustering_filled[df_bond_clustering_filled['ISIN_'] == isin_string].index[0]\n",
    "    \n",
    "    # Calculate Euclidean distances between the selected row and all other rows\n",
    "    distances = euclidean_distances(df_normalized, [df_normalized[index]])\n",
    "    \n",
    "    # Get the indices of the 5 nearest rows (excluding the row itself)\n",
    "    nearest_indices = np.argsort(distances.flatten())[1:6]\n",
    "    \n",
    "    # Retrieve the corresponding rows from the original DataFrame\n",
    "    nearest_rows = df_bond_clustering_filled.iloc[nearest_indices]\n",
    "    \n",
    "    return nearest_rows\n",
    "\n",
    "# Example usage:\n",
    "isin_to_search = 'XS2236363573'\n",
    "result = get_nearest_rows(df_normalized, isin_to_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bond_clustering_filled[df_bond_clustering_filled['ISIN_']=='XS2236363573']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
